{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # Split text into individual words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Get the set of English stopwords\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in english_stopwords]\n",
    "    \n",
    "    # Join the remaining words back into a single string\n",
    "    processed_text = ' '.join(words)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2, ngram_range=(1, 2)):\n",
    "    # Preprocess the texts\n",
    "    text1 = preprocess_text(text1)\n",
    "    text2 = preprocess_text(text2)\n",
    "    \n",
    "    # Combine the preprocessed texts into a list\n",
    "    texts = [text1, text2]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    texts = [remove_stopwords(text) for text in texts]\n",
    "    \n",
    "    # Create an instance of TfidfVectorizer with n-gram support\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    \n",
    "    # Fit and transform the texts to obtain the TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Calculate the cosine similarity between the TF-IDF vectors\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    \n",
    "    return similarity_matrix[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_analyze_texts(pText1, pText2, pText3, pNgram=1):\n",
    "\n",
    "    text1 = pText1\n",
    "    text2 = pText2\n",
    "    text3 = pText3\n",
    "\n",
    "    # Tokenize and preprocess the texts\n",
    "    tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "    tokens1 = tokenizer.tokenize(text1.lower())\n",
    "    tokens2 = tokenizer.tokenize(text2.lower())\n",
    "    tokens3 = tokenizer.tokenize(text3.lower())\n",
    "\n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2, text3])\n",
    "\n",
    "    # Calculate cosine similarity between text1 and text2\n",
    "    similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    print(f\"Similarity between text1 and text2: {similarity[0][0]}\")\n",
    "\n",
    "    # Calculate cosine similarity between text1 and text3\n",
    "    similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[2])\n",
    "    print(f\"Similarity between text1 and text3: {similarity[0][0]}\")\n",
    "\n",
    "    print(\"similarity ngram t1 and t2\")\n",
    "    similarity = calculate_similarity(text1, text2, ngram_range=(1, pNgram))\n",
    "    print(similarity)\n",
    "    \n",
    "    print(\"similarity ngram t1 and t3\")\n",
    "    similarity = calculate_similarity(text1, text3, ngram_range=(1, pNgram))\n",
    "    print(similarity)\n",
    "\n",
    "    return tokens1, tokens2, tokens3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2: 0.7004030841944571\n",
      "Similarity between text1 and text3: 0.1857421473897179\n",
      "similarity ngram t1 and t2\n",
      "0.5101490193104813\n",
      "similarity ngram t1 and t3\n",
      "0.1273595297947935\n",
      "['the', 'old', 'lazy', 'cat', 'is', 'on', 'the', 'mat']\n",
      "['the', 'cat', 'is', 'not', 'on', 'the', 'big', 'old', 'dirty', 'mat']\n",
      "['the', 'big', 'fucking', 'cat', 'and', 'dog', 'are', 'friends']\n"
     ]
    }
   ],
   "source": [
    "# Sample texts\n",
    "text1 = \"The old lazy cat is on the mat\"\n",
    "text2 = \"The cat is not on the big old dirty mat\"\n",
    "text3 = \"The big fucking cat and dog are friends\"\n",
    "tokens1, tokens2, tokens3 = do_analyze_texts (text1, text2, text3)\n",
    "print(tokens1)\n",
    "print(tokens2) \n",
    "print(tokens3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2: 0.8813356859409095\n",
      "Similarity between text1 and text3: 0.2588470024866933\n",
      "similarity ngram t1 and t2\n",
      "1.0000000000000002\n",
      "similarity ngram t1 and t3\n",
      "0.13627634143908643\n",
      "['the', 'cat', 'is', 'on', 'the', 'mat']\n",
      "['the', 'cat', 'is', 'not', 'on', 'the', 'mat']\n",
      "['the', 'cat', 'and', 'dog', 'are', 'friends']\n"
     ]
    }
   ],
   "source": [
    "# Sample texts\n",
    "text1 = \"The cat is on the mat\"\n",
    "text2 = \"The cat is not on the mat\"\n",
    "text3 = \"The cat and dog are friends\"\n",
    "tokens1, tokens2, tokens3 = do_analyze_texts (text1, text2, text3, pNgram=4)\n",
    "print(tokens1)\n",
    "print(tokens2) \n",
    "print(tokens3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2: 1.0\n",
      "Similarity between text1 and text3: 0.23636982151884145\n",
      "similarity ngram t1 and t2\n",
      "1.0000000000000002\n",
      "similarity ngram t1 and t3\n",
      "0.26055567105626243\n",
      "['cat', 'mat']\n",
      "['cat', 'mat']\n",
      "['cat', 'dog', 'friends']\n"
     ]
    }
   ],
   "source": [
    "pre_proc_1 = remove_stopwords(preprocess_text(text1))\n",
    "pre_proc_2 = remove_stopwords(preprocess_text(text2))\n",
    "pre_proc_3 = remove_stopwords(preprocess_text(text3))\n",
    "tokens1, tokens2, tokens3 = do_analyze_texts (pre_proc_1, pre_proc_2, pre_proc_3)\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2) \n",
    "print(tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2: 1.0\n",
      "Similarity between text1 and text3: 0.23636982151884145\n",
      "similarity ngram t1 and t2\n",
      "1.0000000000000002\n",
      "similarity ngram t1 and t3\n",
      "0.13627634143908643\n",
      "['cat', 'mat']\n",
      "['cat', 'mat']\n",
      "['cat', 'dog', 'friends']\n"
     ]
    }
   ],
   "source": [
    "pre_proc_1 = remove_stopwords(preprocess_text(text1))\n",
    "pre_proc_2 = remove_stopwords(preprocess_text(text2))\n",
    "pre_proc_3 = remove_stopwords(preprocess_text(text3))\n",
    "tokens1, tokens2, tokens3 = do_analyze_texts (pre_proc_1, pre_proc_2, pre_proc_3, pNgram=3)\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2) \n",
    "print(tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'is', 'on', 'the', 'mat']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'is', 'not', 'on', 'the', 'mat']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /home/hadoop/.local/lib/python3.10/site-packages (from gensim) (1.23.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m313.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7.0 in /home/hadoop/.local/lib/python3.10/site-packages (from gensim) (1.9.3)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.1 smart-open-6.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2: 0.06408979743719101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sample texts\n",
    "text1 = \"The cat is on the mat\"\n",
    "text2 = \"The cat is not on the mat\"\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokens1 = tokenizer.tokenize(text1.lower())\n",
    "tokens2 = tokenizer.tokenize(text2.lower())\n",
    "\n",
    "# Build vocabulary\n",
    "sentences = [tokens1, tokens2]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=100)\n",
    "\n",
    "# Calculate cosine similarity between text1 and text2 using Word2Vec embeddings\n",
    "similarity = model.wv.similarity('not', 'on')  # Negated word: 'not'\n",
    "print(f\"Similarity between text1 and text2: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
