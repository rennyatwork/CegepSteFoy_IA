{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "count_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the count_matrix to csr_matrix\n",
    "csr_matrix = csr_matrix(count_matrix)\n",
    "\n",
    "# Print the csr_matrix\n",
    "print(csr_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Apriori:\n",
    "    def __init__(self, transactions, min_support, min_confidence):\n",
    "        self.transactions = transactions\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        self.support_data = {}\n",
    "\n",
    "    def generate_C1(self):\n",
    "        C1 = []\n",
    "        for transaction in self.transactions:\n",
    "            for item in transaction:\n",
    "                if [item] not in C1:\n",
    "                    C1.append([item])\n",
    "        C1.sort()\n",
    "        return list(map(frozenset, C1))\n",
    "\n",
    "    def generate_Lk_from_Ck(self, Ck):\n",
    "        item_count = {}\n",
    "        for transaction in self.transactions:\n",
    "            for itemset in Ck:\n",
    "                if itemset.issubset(transaction):\n",
    "                    item_count[itemset] = item_count.get(itemset, 0) + 1\n",
    "\n",
    "        Lk = []\n",
    "        transaction_count = len(self.transactions)\n",
    "        for itemset, count in item_count.items():\n",
    "            support = count / transaction_count\n",
    "            if support >= self.min_support:\n",
    "                Lk.append(itemset)\n",
    "                self.support_data[itemset] = support\n",
    "\n",
    "        return Lk\n",
    "\n",
    "    def generate_Ck(self, Lk_1, k):\n",
    "        Ck = []\n",
    "        len_Lk_1 = len(Lk_1)\n",
    "        for i in range(len_Lk_1):\n",
    "            for j in range(i + 1, len_Lk_1):\n",
    "                itemset1 = list(Lk_1[i])\n",
    "                itemset2 = list(Lk_1[j])\n",
    "                itemset1.sort()\n",
    "                itemset2.sort()\n",
    "\n",
    "                # Check if the first k-2 items are the same\n",
    "                if itemset1[:k-2] == itemset2[:k-2]:\n",
    "                    # Create a new candidate itemset by merging itemset1 and itemset2\n",
    "                    new_itemset = frozenset(itemset1 + [itemset2[-1]])\n",
    "                    Ck.append(new_itemset)\n",
    "        return Ck\n",
    "\n",
    "    def apriori(self):\n",
    "        C1 = self.generate_C1()\n",
    "        L1 = self.generate_Lk_from_Ck(C1)\n",
    "        k = 2\n",
    "        frequent_itemsets = [L1]\n",
    "        while len(frequent_itemsets[k-2]) > 0:\n",
    "            Ck = self.generate_Ck(frequent_itemsets[k-2], k)\n",
    "            Lk = self.generate_Lk_from_Ck(Ck)\n",
    "            frequent_itemsets.append(Lk)\n",
    "            k += 1\n",
    "\n",
    "        return frequent_itemsets\n",
    "\n",
    "\n",
    "# Example usage\n",
    "lst_trans = [\n",
    "    [1, 3, 4],\n",
    "    [2, 3, 5],\n",
    "    [1, 2, 3, 5],\n",
    "    [2, 5]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: [frozenset({1}), frozenset({3}), frozenset({2}), frozenset({5})]\n",
      "L2: [frozenset({1, 3}), frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5})]\n",
      "L3: [frozenset({2, 3, 5})]\n",
      "L4: []\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "lst_trans = [\n",
    "    [1, 3, 4],\n",
    "    [2, 3, 5],\n",
    "    [1, 2, 3, 5],\n",
    "    [2, 5]\n",
    "]\n",
    "\n",
    "model = Apriori(transactions=lst_trans, min_support=0.3, min_confidence=0.7)\n",
    "frequent_itemsets = model.apriori()\n",
    "\n",
    "for i, Lk in enumerate(frequent_itemsets):\n",
    "    print(\"L{}: {}\".format(i+1, Lk))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
