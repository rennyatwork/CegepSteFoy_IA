{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rennyatwork/CegepSteFoy_IA/blob/main/08_deep_learnin/RNN_Debit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importation des packages"
      ],
      "metadata": {
        "id": "YJSKonmfIOzj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nnTT-iHdHFfF"
      },
      "outputs": [],
      "source": [
        "#Importation des packages\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Chargement du jeu de données contenant les température, quantité de précipitations et débit de la rivière Saint-Charles pour quelques jours"
      ],
      "metadata": {
        "id": "H25DL5FFIbel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = read_csv('https://raw.githubusercontent.com/rennyatwork/CegepSteFoy_IA/main/08_deep_learning/data/Debit_St-Charles.csv')\n",
        "print(len(df))\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "EQ2pCJE9IcOR",
        "outputId": "05ea6eb0-7e14-4669-91bb-bd2d0a602045"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "119\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     An  Mois  Jour  Temp  Precip  Debit\n",
              "0  2023     4    13   7.2     0.0  17.41\n",
              "1  2023     4    14   6.4     0.0  25.37\n",
              "2  2023     4    15   5.7     0.0  30.38\n",
              "3  2023     4    16   7.6     0.0  36.74\n",
              "4  2023     4    17   5.9    11.8  51.50\n",
              "5  2023     4    18   5.2     0.7  69.64\n",
              "6  2023     4    19   4.9     1.7  72.08\n",
              "7  2023     4    20   2.6     0.0  57.16\n",
              "8  2023     4    21   3.5     0.0  42.24\n",
              "9  2023     4    22  10.2     0.0  34.46"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fa12f58-1623-43de-a4eb-8e562e5f2f9a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>An</th>\n",
              "      <th>Mois</th>\n",
              "      <th>Jour</th>\n",
              "      <th>Temp</th>\n",
              "      <th>Precip</th>\n",
              "      <th>Debit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>6.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>5.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>7.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>5.9</td>\n",
              "      <td>11.8</td>\n",
              "      <td>51.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>5.2</td>\n",
              "      <td>0.7</td>\n",
              "      <td>69.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>4.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>72.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "      <td>10.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fa12f58-1623-43de-a4eb-8e562e5f2f9a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7fa12f58-1623-43de-a4eb-8e562e5f2f9a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7fa12f58-1623-43de-a4eb-8e562e5f2f9a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c4e94341-6989-41b2-b1cd-d24b913ddae0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c4e94341-6989-41b2-b1cd-d24b913ddae0')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c4e94341-6989-41b2-b1cd-d24b913ddae0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 1: Série unidimensionnelle: nous allons commencer par tenter de prédire les débits en fonction uniquement des débits passés"
      ],
      "metadata": {
        "id": "SVWr_xqXtBNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Fonction servant à formatter les données d'un séquance unidimensionnelle"
      ],
      "metadata": {
        "id": "1CyAVXdMJDl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sequence(sequence, n_steps):\n",
        " X, y = list(), list()\n",
        " for i in range(len(sequence)):\n",
        " # find the end of this pattern\n",
        "  end_ix = i + n_steps\n",
        " # check if we are beyond the sequence\n",
        "  if end_ix > len(sequence)-1:\n",
        "    break\n",
        " # gather input and output parts of the pattern\n",
        "  seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "  X.append(seq_x)\n",
        "  y.append(seq_y)\n",
        " return array(X), array(y)"
      ],
      "metadata": {
        "id": "ycVPtOpOJKaD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Définir la séquence des débits"
      ],
      "metadata": {
        "id": "ErjUqB9kJwqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_seq = np.array(df)\n",
        "raw_seq = raw_seq[:,5]\n",
        "raw_seq = raw_seq.astype(np.float32)\n",
        "print(raw_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edtgsANyJ2F2",
        "outputId": "2a25ac6e-2c50-44d6-c659-3e256b3f4f8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17.41  25.37  30.38  36.74  51.5   69.64  72.08  57.16  42.24  34.46\n",
            " 37.96  48.31  58.88  53.85  45.15  37.22  31.66  32.68  42.2   63.83\n",
            " 55.17  38.87  24.79  17.85  15.75  13.19   9.438  8.42   6.156  5.106\n",
            "  6.626  5.63   4.519  4.226  4.337  3.58   3.411  3.469  6.423  7.25\n",
            "  5.621  4.085  3.882  3.391  3.082  2.755  2.36   2.055  1.831  1.961\n",
            "  2.449  2.189  1.857  1.59   1.617  1.908  1.93   1.987  1.525  1.478\n",
            "  1.261  1.832  2.562  3.053  3.262  3.716  3.651  3.067  2.844  2.336\n",
            "  1.84   1.528  1.514  1.554  1.146  1.404  2.057  6.54  10.23   7.732\n",
            " 11.44  13.41   9.968  5.758  3.485  2.562  2.538  3.102 18.4   45.4\n",
            " 62.69  55.89  53.82  49.78  39.02  39.25  33.63  21.36  13.94   8.779\n",
            "  9.718 10.73   9.286  7.464  6.291  5.438  5.994  6.074  5.497  5.776\n",
            "  5.11   3.651  3.636 12.38  15.68  10.12   7.821 31.46  39.06 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Formatter la séquence des débits pour l'analyse LSTM à l'aide de la fonction split_sequence()"
      ],
      "metadata": {
        "id": "QDioQnHRKfQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 3\n",
        "\n",
        "X, y = split_sequence(raw_seq, n_steps)\n",
        "\n",
        "n_features = 1\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "for i in range(len(X)):\n",
        " print(X[i], y[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouGHmfIxKnU3",
        "outputId": "75e662ed-ff08-4f79-d3f7-d29edf1dd731"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(116, 3, 1) (116,)\n",
            "[[17.41]\n",
            " [25.37]\n",
            " [30.38]] 36.74\n",
            "[[25.37]\n",
            " [30.38]\n",
            " [36.74]] 51.5\n",
            "[[30.38]\n",
            " [36.74]\n",
            " [51.5 ]] 69.64\n",
            "[[36.74]\n",
            " [51.5 ]\n",
            " [69.64]] 72.08\n",
            "[[51.5 ]\n",
            " [69.64]\n",
            " [72.08]] 57.16\n",
            "[[69.64]\n",
            " [72.08]\n",
            " [57.16]] 42.24\n",
            "[[72.08]\n",
            " [57.16]\n",
            " [42.24]] 34.46\n",
            "[[57.16]\n",
            " [42.24]\n",
            " [34.46]] 37.96\n",
            "[[42.24]\n",
            " [34.46]\n",
            " [37.96]] 48.31\n",
            "[[34.46]\n",
            " [37.96]\n",
            " [48.31]] 58.88\n",
            "[[37.96]\n",
            " [48.31]\n",
            " [58.88]] 53.85\n",
            "[[48.31]\n",
            " [58.88]\n",
            " [53.85]] 45.15\n",
            "[[58.88]\n",
            " [53.85]\n",
            " [45.15]] 37.22\n",
            "[[53.85]\n",
            " [45.15]\n",
            " [37.22]] 31.66\n",
            "[[45.15]\n",
            " [37.22]\n",
            " [31.66]] 32.68\n",
            "[[37.22]\n",
            " [31.66]\n",
            " [32.68]] 42.2\n",
            "[[31.66]\n",
            " [32.68]\n",
            " [42.2 ]] 63.83\n",
            "[[32.68]\n",
            " [42.2 ]\n",
            " [63.83]] 55.17\n",
            "[[42.2 ]\n",
            " [63.83]\n",
            " [55.17]] 38.87\n",
            "[[63.83]\n",
            " [55.17]\n",
            " [38.87]] 24.79\n",
            "[[55.17]\n",
            " [38.87]\n",
            " [24.79]] 17.85\n",
            "[[38.87]\n",
            " [24.79]\n",
            " [17.85]] 15.75\n",
            "[[24.79]\n",
            " [17.85]\n",
            " [15.75]] 13.19\n",
            "[[17.85]\n",
            " [15.75]\n",
            " [13.19]] 9.438\n",
            "[[15.75 ]\n",
            " [13.19 ]\n",
            " [ 9.438]] 8.42\n",
            "[[13.19 ]\n",
            " [ 9.438]\n",
            " [ 8.42 ]] 6.156\n",
            "[[9.438]\n",
            " [8.42 ]\n",
            " [6.156]] 5.106\n",
            "[[8.42 ]\n",
            " [6.156]\n",
            " [5.106]] 6.626\n",
            "[[6.156]\n",
            " [5.106]\n",
            " [6.626]] 5.63\n",
            "[[5.106]\n",
            " [6.626]\n",
            " [5.63 ]] 4.519\n",
            "[[6.626]\n",
            " [5.63 ]\n",
            " [4.519]] 4.226\n",
            "[[5.63 ]\n",
            " [4.519]\n",
            " [4.226]] 4.337\n",
            "[[4.519]\n",
            " [4.226]\n",
            " [4.337]] 3.58\n",
            "[[4.226]\n",
            " [4.337]\n",
            " [3.58 ]] 3.411\n",
            "[[4.337]\n",
            " [3.58 ]\n",
            " [3.411]] 3.469\n",
            "[[3.58 ]\n",
            " [3.411]\n",
            " [3.469]] 6.423\n",
            "[[3.411]\n",
            " [3.469]\n",
            " [6.423]] 7.25\n",
            "[[3.469]\n",
            " [6.423]\n",
            " [7.25 ]] 5.621\n",
            "[[6.423]\n",
            " [7.25 ]\n",
            " [5.621]] 4.085\n",
            "[[7.25 ]\n",
            " [5.621]\n",
            " [4.085]] 3.882\n",
            "[[5.621]\n",
            " [4.085]\n",
            " [3.882]] 3.391\n",
            "[[4.085]\n",
            " [3.882]\n",
            " [3.391]] 3.082\n",
            "[[3.882]\n",
            " [3.391]\n",
            " [3.082]] 2.755\n",
            "[[3.391]\n",
            " [3.082]\n",
            " [2.755]] 2.36\n",
            "[[3.082]\n",
            " [2.755]\n",
            " [2.36 ]] 2.055\n",
            "[[2.755]\n",
            " [2.36 ]\n",
            " [2.055]] 1.831\n",
            "[[2.36 ]\n",
            " [2.055]\n",
            " [1.831]] 1.961\n",
            "[[2.055]\n",
            " [1.831]\n",
            " [1.961]] 2.449\n",
            "[[1.831]\n",
            " [1.961]\n",
            " [2.449]] 2.189\n",
            "[[1.961]\n",
            " [2.449]\n",
            " [2.189]] 1.857\n",
            "[[2.449]\n",
            " [2.189]\n",
            " [1.857]] 1.59\n",
            "[[2.189]\n",
            " [1.857]\n",
            " [1.59 ]] 1.617\n",
            "[[1.857]\n",
            " [1.59 ]\n",
            " [1.617]] 1.908\n",
            "[[1.59 ]\n",
            " [1.617]\n",
            " [1.908]] 1.93\n",
            "[[1.617]\n",
            " [1.908]\n",
            " [1.93 ]] 1.987\n",
            "[[1.908]\n",
            " [1.93 ]\n",
            " [1.987]] 1.525\n",
            "[[1.93 ]\n",
            " [1.987]\n",
            " [1.525]] 1.478\n",
            "[[1.987]\n",
            " [1.525]\n",
            " [1.478]] 1.261\n",
            "[[1.525]\n",
            " [1.478]\n",
            " [1.261]] 1.832\n",
            "[[1.478]\n",
            " [1.261]\n",
            " [1.832]] 2.562\n",
            "[[1.261]\n",
            " [1.832]\n",
            " [2.562]] 3.053\n",
            "[[1.832]\n",
            " [2.562]\n",
            " [3.053]] 3.262\n",
            "[[2.562]\n",
            " [3.053]\n",
            " [3.262]] 3.716\n",
            "[[3.053]\n",
            " [3.262]\n",
            " [3.716]] 3.651\n",
            "[[3.262]\n",
            " [3.716]\n",
            " [3.651]] 3.067\n",
            "[[3.716]\n",
            " [3.651]\n",
            " [3.067]] 2.844\n",
            "[[3.651]\n",
            " [3.067]\n",
            " [2.844]] 2.336\n",
            "[[3.067]\n",
            " [2.844]\n",
            " [2.336]] 1.84\n",
            "[[2.844]\n",
            " [2.336]\n",
            " [1.84 ]] 1.528\n",
            "[[2.336]\n",
            " [1.84 ]\n",
            " [1.528]] 1.514\n",
            "[[1.84 ]\n",
            " [1.528]\n",
            " [1.514]] 1.554\n",
            "[[1.528]\n",
            " [1.514]\n",
            " [1.554]] 1.146\n",
            "[[1.514]\n",
            " [1.554]\n",
            " [1.146]] 1.404\n",
            "[[1.554]\n",
            " [1.146]\n",
            " [1.404]] 2.057\n",
            "[[1.146]\n",
            " [1.404]\n",
            " [2.057]] 6.54\n",
            "[[1.404]\n",
            " [2.057]\n",
            " [6.54 ]] 10.23\n",
            "[[ 2.057]\n",
            " [ 6.54 ]\n",
            " [10.23 ]] 7.732\n",
            "[[ 6.54 ]\n",
            " [10.23 ]\n",
            " [ 7.732]] 11.44\n",
            "[[10.23 ]\n",
            " [ 7.732]\n",
            " [11.44 ]] 13.41\n",
            "[[ 7.732]\n",
            " [11.44 ]\n",
            " [13.41 ]] 9.968\n",
            "[[11.44 ]\n",
            " [13.41 ]\n",
            " [ 9.968]] 5.758\n",
            "[[13.41 ]\n",
            " [ 9.968]\n",
            " [ 5.758]] 3.485\n",
            "[[9.968]\n",
            " [5.758]\n",
            " [3.485]] 2.562\n",
            "[[5.758]\n",
            " [3.485]\n",
            " [2.562]] 2.538\n",
            "[[3.485]\n",
            " [2.562]\n",
            " [2.538]] 3.102\n",
            "[[2.562]\n",
            " [2.538]\n",
            " [3.102]] 18.4\n",
            "[[ 2.538]\n",
            " [ 3.102]\n",
            " [18.4  ]] 45.4\n",
            "[[ 3.102]\n",
            " [18.4  ]\n",
            " [45.4  ]] 62.69\n",
            "[[18.4 ]\n",
            " [45.4 ]\n",
            " [62.69]] 55.89\n",
            "[[45.4 ]\n",
            " [62.69]\n",
            " [55.89]] 53.82\n",
            "[[62.69]\n",
            " [55.89]\n",
            " [53.82]] 49.78\n",
            "[[55.89]\n",
            " [53.82]\n",
            " [49.78]] 39.02\n",
            "[[53.82]\n",
            " [49.78]\n",
            " [39.02]] 39.25\n",
            "[[49.78]\n",
            " [39.02]\n",
            " [39.25]] 33.63\n",
            "[[39.02]\n",
            " [39.25]\n",
            " [33.63]] 21.36\n",
            "[[39.25]\n",
            " [33.63]\n",
            " [21.36]] 13.94\n",
            "[[33.63]\n",
            " [21.36]\n",
            " [13.94]] 8.779\n",
            "[[21.36 ]\n",
            " [13.94 ]\n",
            " [ 8.779]] 9.718\n",
            "[[13.94 ]\n",
            " [ 8.779]\n",
            " [ 9.718]] 10.73\n",
            "[[ 8.779]\n",
            " [ 9.718]\n",
            " [10.73 ]] 9.286\n",
            "[[ 9.718]\n",
            " [10.73 ]\n",
            " [ 9.286]] 7.464\n",
            "[[10.73 ]\n",
            " [ 9.286]\n",
            " [ 7.464]] 6.291\n",
            "[[9.286]\n",
            " [7.464]\n",
            " [6.291]] 5.438\n",
            "[[7.464]\n",
            " [6.291]\n",
            " [5.438]] 5.994\n",
            "[[6.291]\n",
            " [5.438]\n",
            " [5.994]] 6.074\n",
            "[[5.438]\n",
            " [5.994]\n",
            " [6.074]] 5.497\n",
            "[[5.994]\n",
            " [6.074]\n",
            " [5.497]] 5.776\n",
            "[[6.074]\n",
            " [5.497]\n",
            " [5.776]] 5.11\n",
            "[[5.497]\n",
            " [5.776]\n",
            " [5.11 ]] 3.651\n",
            "[[5.776]\n",
            " [5.11 ]\n",
            " [3.651]] 3.636\n",
            "[[5.11 ]\n",
            " [3.651]\n",
            " [3.636]] 12.38\n",
            "[[ 3.651]\n",
            " [ 3.636]\n",
            " [12.38 ]] 15.68\n",
            "[[ 3.636]\n",
            " [12.38 ]\n",
            " [15.68 ]] 10.12\n",
            "[[12.38]\n",
            " [15.68]\n",
            " [10.12]] 7.821\n",
            "[[15.68 ]\n",
            " [10.12 ]\n",
            " [ 7.821]] 31.46\n",
            "[[10.12 ]\n",
            " [ 7.821]\n",
            " [31.46 ]] 39.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Diviser les données en train, valid et test data"
      ],
      "metadata": {
        "id": "rmzJhLAyMDkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = X[:70], y[:70]\n",
        "X_valid, y_valid = X[70:80], y[70:80]\n",
        "X_test, y_test = X[80:], y[80:]\n",
        "\n",
        "print(X_train.shape, X_valid.shape, X_test.shape)\n",
        "\n",
        "for i in range(len(X_train)):\n",
        " print(X_train[i], y_train[i])\n",
        "\n",
        "#for i in range(len(X_test)):\n",
        "# print(X_test[i], y_test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCvYk68VMIsD",
        "outputId": "a7aa1bd6-4f5c-483e-ed5d-8aaded637d53"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 3, 1) (10, 3, 1) (36, 3, 1)\n",
            "[[17.41]\n",
            " [25.37]\n",
            " [30.38]] 36.74\n",
            "[[25.37]\n",
            " [30.38]\n",
            " [36.74]] 51.5\n",
            "[[30.38]\n",
            " [36.74]\n",
            " [51.5 ]] 69.64\n",
            "[[36.74]\n",
            " [51.5 ]\n",
            " [69.64]] 72.08\n",
            "[[51.5 ]\n",
            " [69.64]\n",
            " [72.08]] 57.16\n",
            "[[69.64]\n",
            " [72.08]\n",
            " [57.16]] 42.24\n",
            "[[72.08]\n",
            " [57.16]\n",
            " [42.24]] 34.46\n",
            "[[57.16]\n",
            " [42.24]\n",
            " [34.46]] 37.96\n",
            "[[42.24]\n",
            " [34.46]\n",
            " [37.96]] 48.31\n",
            "[[34.46]\n",
            " [37.96]\n",
            " [48.31]] 58.88\n",
            "[[37.96]\n",
            " [48.31]\n",
            " [58.88]] 53.85\n",
            "[[48.31]\n",
            " [58.88]\n",
            " [53.85]] 45.15\n",
            "[[58.88]\n",
            " [53.85]\n",
            " [45.15]] 37.22\n",
            "[[53.85]\n",
            " [45.15]\n",
            " [37.22]] 31.66\n",
            "[[45.15]\n",
            " [37.22]\n",
            " [31.66]] 32.68\n",
            "[[37.22]\n",
            " [31.66]\n",
            " [32.68]] 42.2\n",
            "[[31.66]\n",
            " [32.68]\n",
            " [42.2 ]] 63.83\n",
            "[[32.68]\n",
            " [42.2 ]\n",
            " [63.83]] 55.17\n",
            "[[42.2 ]\n",
            " [63.83]\n",
            " [55.17]] 38.87\n",
            "[[63.83]\n",
            " [55.17]\n",
            " [38.87]] 24.79\n",
            "[[55.17]\n",
            " [38.87]\n",
            " [24.79]] 17.85\n",
            "[[38.87]\n",
            " [24.79]\n",
            " [17.85]] 15.75\n",
            "[[24.79]\n",
            " [17.85]\n",
            " [15.75]] 13.19\n",
            "[[17.85]\n",
            " [15.75]\n",
            " [13.19]] 9.438\n",
            "[[15.75 ]\n",
            " [13.19 ]\n",
            " [ 9.438]] 8.42\n",
            "[[13.19 ]\n",
            " [ 9.438]\n",
            " [ 8.42 ]] 6.156\n",
            "[[9.438]\n",
            " [8.42 ]\n",
            " [6.156]] 5.106\n",
            "[[8.42 ]\n",
            " [6.156]\n",
            " [5.106]] 6.626\n",
            "[[6.156]\n",
            " [5.106]\n",
            " [6.626]] 5.63\n",
            "[[5.106]\n",
            " [6.626]\n",
            " [5.63 ]] 4.519\n",
            "[[6.626]\n",
            " [5.63 ]\n",
            " [4.519]] 4.226\n",
            "[[5.63 ]\n",
            " [4.519]\n",
            " [4.226]] 4.337\n",
            "[[4.519]\n",
            " [4.226]\n",
            " [4.337]] 3.58\n",
            "[[4.226]\n",
            " [4.337]\n",
            " [3.58 ]] 3.411\n",
            "[[4.337]\n",
            " [3.58 ]\n",
            " [3.411]] 3.469\n",
            "[[3.58 ]\n",
            " [3.411]\n",
            " [3.469]] 6.423\n",
            "[[3.411]\n",
            " [3.469]\n",
            " [6.423]] 7.25\n",
            "[[3.469]\n",
            " [6.423]\n",
            " [7.25 ]] 5.621\n",
            "[[6.423]\n",
            " [7.25 ]\n",
            " [5.621]] 4.085\n",
            "[[7.25 ]\n",
            " [5.621]\n",
            " [4.085]] 3.882\n",
            "[[5.621]\n",
            " [4.085]\n",
            " [3.882]] 3.391\n",
            "[[4.085]\n",
            " [3.882]\n",
            " [3.391]] 3.082\n",
            "[[3.882]\n",
            " [3.391]\n",
            " [3.082]] 2.755\n",
            "[[3.391]\n",
            " [3.082]\n",
            " [2.755]] 2.36\n",
            "[[3.082]\n",
            " [2.755]\n",
            " [2.36 ]] 2.055\n",
            "[[2.755]\n",
            " [2.36 ]\n",
            " [2.055]] 1.831\n",
            "[[2.36 ]\n",
            " [2.055]\n",
            " [1.831]] 1.961\n",
            "[[2.055]\n",
            " [1.831]\n",
            " [1.961]] 2.449\n",
            "[[1.831]\n",
            " [1.961]\n",
            " [2.449]] 2.189\n",
            "[[1.961]\n",
            " [2.449]\n",
            " [2.189]] 1.857\n",
            "[[2.449]\n",
            " [2.189]\n",
            " [1.857]] 1.59\n",
            "[[2.189]\n",
            " [1.857]\n",
            " [1.59 ]] 1.617\n",
            "[[1.857]\n",
            " [1.59 ]\n",
            " [1.617]] 1.908\n",
            "[[1.59 ]\n",
            " [1.617]\n",
            " [1.908]] 1.93\n",
            "[[1.617]\n",
            " [1.908]\n",
            " [1.93 ]] 1.987\n",
            "[[1.908]\n",
            " [1.93 ]\n",
            " [1.987]] 1.525\n",
            "[[1.93 ]\n",
            " [1.987]\n",
            " [1.525]] 1.478\n",
            "[[1.987]\n",
            " [1.525]\n",
            " [1.478]] 1.261\n",
            "[[1.525]\n",
            " [1.478]\n",
            " [1.261]] 1.832\n",
            "[[1.478]\n",
            " [1.261]\n",
            " [1.832]] 2.562\n",
            "[[1.261]\n",
            " [1.832]\n",
            " [2.562]] 3.053\n",
            "[[1.832]\n",
            " [2.562]\n",
            " [3.053]] 3.262\n",
            "[[2.562]\n",
            " [3.053]\n",
            " [3.262]] 3.716\n",
            "[[3.053]\n",
            " [3.262]\n",
            " [3.716]] 3.651\n",
            "[[3.262]\n",
            " [3.716]\n",
            " [3.651]] 3.067\n",
            "[[3.716]\n",
            " [3.651]\n",
            " [3.067]] 2.844\n",
            "[[3.651]\n",
            " [3.067]\n",
            " [2.844]] 2.336\n",
            "[[3.067]\n",
            " [2.844]\n",
            " [2.336]] 1.84\n",
            "[[2.844]\n",
            " [2.336]\n",
            " [1.84 ]] 1.528\n",
            "[[2.336]\n",
            " [1.84 ]\n",
            " [1.528]] 1.514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Définir un premier modèle LSTM univarié"
      ],
      "metadata": {
        "id": "jM9DZonvODQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(150, activation='relu', kernel_initializer=\"he_normal\",input_shape=(n_steps, n_features)))\n",
        "model.add(Dense(50,activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50,activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "jMQb2saLOGuo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Entrainer le modèle"
      ],
      "metadata": {
        "id": "MW6AGLDOPnx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=1, validation_data=[X_valid,y_valid])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv_5XiLCPpsy",
        "outputId": "a4db191f-7141-4494-a125-084538601887"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3/3 [==============================] - 4s 204ms/step - loss: 2838.2251 - val_loss: 10.2471\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 435.3116 - val_loss: 25.7746\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 490.1110 - val_loss: 40.7724\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 541.1368 - val_loss: 39.7068\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 515.2003 - val_loss: 29.7596\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 303.1404 - val_loss: 22.6505\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 394.2332 - val_loss: 19.5781\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 442.9881 - val_loss: 17.6552\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 253.8015 - val_loss: 17.0615\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 289.9391 - val_loss: 16.9022\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 241.9057 - val_loss: 16.9781\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 243.8925 - val_loss: 16.3301\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 116.5099 - val_loss: 16.3070\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 203.2503 - val_loss: 16.9046\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 125.3681 - val_loss: 17.7036\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 228.0300 - val_loss: 17.5121\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 176.7090 - val_loss: 15.8136\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 204.8636 - val_loss: 13.3986\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 96.5773 - val_loss: 11.8507\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 127.2529 - val_loss: 11.4773\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 119.7443 - val_loss: 11.4998\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 104.0464 - val_loss: 11.9675\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 145.6749 - val_loss: 13.1123\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 131.6269 - val_loss: 13.8493\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 164.2276 - val_loss: 13.7892\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 130.9214 - val_loss: 12.7199\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 149.2701 - val_loss: 11.6958\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 182.4523 - val_loss: 11.3849\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 146.9135 - val_loss: 11.3245\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 87.5361 - val_loss: 10.5988\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 106.7700 - val_loss: 9.6766\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 149.2650 - val_loss: 9.0040\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 122.1698 - val_loss: 8.8105\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 102.5954 - val_loss: 9.0933\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 82.2607 - val_loss: 9.7259\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 124.8270 - val_loss: 10.1125\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 92.6340 - val_loss: 10.2035\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 60.4719 - val_loss: 9.9656\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 89.1854 - val_loss: 9.8273\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 54.6187 - val_loss: 9.6864\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 61.7678 - val_loss: 9.5796\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 91.5738 - val_loss: 9.7425\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 91.4097 - val_loss: 9.7710\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 47.0644 - val_loss: 9.5122\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 41.2228 - val_loss: 9.3796\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 60.2923 - val_loss: 9.4587\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 74.3433 - val_loss: 9.6229\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 52.1214 - val_loss: 10.1169\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 53.9944 - val_loss: 11.3046\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 76.6454 - val_loss: 10.7734\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 55.3140 - val_loss: 9.9701\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 147.7909 - val_loss: 10.1958\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 100.2297 - val_loss: 9.9930\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 85.3242 - val_loss: 9.1513\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 57.9621 - val_loss: 8.9555\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 46.7051 - val_loss: 9.1556\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 47.0550 - val_loss: 9.4657\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 55.7530 - val_loss: 9.9490\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 85.3690 - val_loss: 10.5691\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 72.6889 - val_loss: 10.7898\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 38.9778 - val_loss: 10.7993\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 108.4824 - val_loss: 10.7405\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 66.3316 - val_loss: 10.5783\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 36.0184 - val_loss: 10.2896\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 57.9519 - val_loss: 10.0119\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 77.0240 - val_loss: 10.0199\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 78.7389 - val_loss: 10.1989\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 60.6840 - val_loss: 10.3309\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 33.2855 - val_loss: 10.3386\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 56.9164 - val_loss: 10.6503\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 61.5724 - val_loss: 10.7447\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 39.4229 - val_loss: 10.5936\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 58.7114 - val_loss: 10.4480\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 24.0827 - val_loss: 10.2826\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 52.5594 - val_loss: 9.9070\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 50.9838 - val_loss: 9.5575\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 45.6586 - val_loss: 9.5487\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 41.5264 - val_loss: 9.9231\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 69.3548 - val_loss: 10.0985\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 54.1783 - val_loss: 9.9075\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 39.0483 - val_loss: 9.6476\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 36.9964 - val_loss: 9.6261\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 46.5847 - val_loss: 9.5815\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 38.3234 - val_loss: 9.6041\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 27.3947 - val_loss: 9.6497\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 24.7732 - val_loss: 10.2872\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 57.5983 - val_loss: 10.7232\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 44.9818 - val_loss: 10.1425\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 77.1340 - val_loss: 9.4601\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 41.6691 - val_loss: 9.0419\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 57.0848 - val_loss: 8.9688\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 42.3751 - val_loss: 9.0517\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 36.9800 - val_loss: 9.3287\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 42.2097 - val_loss: 9.7705\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 37.4208 - val_loss: 10.0524\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 73.3597 - val_loss: 9.8971\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 71.5104 - val_loss: 9.5357\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 64.1782 - val_loss: 9.2709\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 43.8557 - val_loss: 8.8975\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 74.1463 - val_loss: 8.7629\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 89.9454 - val_loss: 8.9734\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 40.6031 - val_loss: 9.4222\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 43.1214 - val_loss: 9.9989\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 65.6825 - val_loss: 10.2277\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 56.0472 - val_loss: 10.5457\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 30.6728 - val_loss: 10.4491\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 40.8176 - val_loss: 10.3459\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 55.7696 - val_loss: 10.3184\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 41.2032 - val_loss: 10.0506\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 42.2817 - val_loss: 9.7596\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 31.5806 - val_loss: 9.6383\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 45.0630 - val_loss: 9.6411\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 42.5897 - val_loss: 9.4898\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 47.6622 - val_loss: 9.2157\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 40.4390 - val_loss: 9.5287\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 60.6464 - val_loss: 10.1742\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 52.5353 - val_loss: 10.2425\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 43.8234 - val_loss: 10.2626\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 54.9692 - val_loss: 10.0704\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 58.9757 - val_loss: 9.2102\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 56.4933 - val_loss: 8.3889\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 39.7928 - val_loss: 8.0209\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 42.1369 - val_loss: 8.0327\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 32.7705 - val_loss: 8.1552\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 30.0664 - val_loss: 8.3425\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 59.9012 - val_loss: 8.7032\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 66.9211 - val_loss: 8.7014\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 60.8138 - val_loss: 8.4136\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.2480 - val_loss: 8.2518\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 45.9469 - val_loss: 8.1503\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 32.4093 - val_loss: 8.1237\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 53.2684 - val_loss: 8.1172\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 40.9309 - val_loss: 8.2583\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 49.9511 - val_loss: 8.5733\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 67.2570 - val_loss: 8.8155\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 32.0777 - val_loss: 9.0638\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 26.3474 - val_loss: 9.3024\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.3562 - val_loss: 9.5444\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 32.7420 - val_loss: 9.7076\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 60.4850 - val_loss: 9.9155\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 70.6500 - val_loss: 9.9220\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 32.6965 - val_loss: 9.5856\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 52.5461 - val_loss: 10.0403\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 27.5552 - val_loss: 11.3202\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 43.8306 - val_loss: 12.0694\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 79.6620 - val_loss: 11.1513\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 37.6015 - val_loss: 10.2555\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 37.3974 - val_loss: 9.7978\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 30.5821 - val_loss: 9.6756\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 53.2090 - val_loss: 10.0116\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.7007 - val_loss: 10.4671\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 75.7001 - val_loss: 10.4169\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 24.0036 - val_loss: 10.3523\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 69.5746 - val_loss: 10.6445\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.9640 - val_loss: 12.0304\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 68.0834 - val_loss: 12.3482\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 39.8688 - val_loss: 12.0820\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 58.3255 - val_loss: 10.9035\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 37.2517 - val_loss: 9.4028\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 55.1966 - val_loss: 8.6767\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 46.9776 - val_loss: 9.1781\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 41.2074 - val_loss: 10.3559\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 52.6310 - val_loss: 11.1072\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 49.1584 - val_loss: 11.6927\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 46.3061 - val_loss: 10.9762\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 30.4862 - val_loss: 10.1835\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 19.2544 - val_loss: 10.0209\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 36.5633 - val_loss: 9.7092\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 49.9873 - val_loss: 9.3743\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 64.5422 - val_loss: 9.3233\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 25.9414 - val_loss: 9.7903\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 48.2882 - val_loss: 10.1275\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 41.7328 - val_loss: 9.4715\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 27.7789 - val_loss: 8.8836\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 22.0896 - val_loss: 8.8269\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 38.5075 - val_loss: 9.0782\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 23.2201 - val_loss: 9.3948\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 56.5967 - val_loss: 9.8759\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 25.0682 - val_loss: 10.1942\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 34.2062 - val_loss: 9.7382\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 55.0599 - val_loss: 9.1077\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 23.0513 - val_loss: 8.5203\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 63.5257 - val_loss: 8.5746\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 23.7910 - val_loss: 9.3017\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 27.2058 - val_loss: 9.9057\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 23.6993 - val_loss: 10.1049\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 11.9752 - val_loss: 9.9440\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 23.0983 - val_loss: 9.4531\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.2936 - val_loss: 8.8936\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 30.7165 - val_loss: 8.5840\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 43.0359 - val_loss: 9.2258\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 19.8465 - val_loss: 9.7128\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 16.5369 - val_loss: 10.0254\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 48.0744 - val_loss: 9.7385\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 33.9083 - val_loss: 9.2059\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 35.8796 - val_loss: 8.6893\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 43.3188 - val_loss: 8.5014\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 34.7233 - val_loss: 8.2855\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 18.5524 - val_loss: 8.4056\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 28.2618 - val_loss: 8.5066\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dea7427f340>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Évaluer le modèle"
      ],
      "metadata": {
        "id": "BHEFCRMyPyR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss  = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um7s-dFmP0s-",
        "outputId": "35c889d9-6d78-4ee8-9d1d-6ed35bccc1f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.69373321533203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Regarder les prévisions pour les données de tests"
      ],
      "metadata": {
        "id": "I29aVt2jgf1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test, verbose=0)\n",
        "\n",
        "for i in range(len(X_test)):\n",
        " print(X_test[i], y_test[i], y_pred[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb5MNK4UgjdG",
        "outputId": "114ab776-5f3f-4c72-bc2a-4a4f2899c609"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[11.44 ]\n",
            " [13.41 ]\n",
            " [ 9.968]] 5.758 [7.0750556]\n",
            "[[13.41 ]\n",
            " [ 9.968]\n",
            " [ 5.758]] 3.485 [3.3405106]\n",
            "[[9.968]\n",
            " [5.758]\n",
            " [3.485]] 2.562 [1.169691]\n",
            "[[5.758]\n",
            " [3.485]\n",
            " [2.562]] 2.538 [1.6986424]\n",
            "[[3.485]\n",
            " [2.562]\n",
            " [2.538]] 3.102 [2.254063]\n",
            "[[2.562]\n",
            " [2.538]\n",
            " [3.102]] 18.4 [3.0376458]\n",
            "[[ 2.538]\n",
            " [ 3.102]\n",
            " [18.4  ]] 45.4 [15.784789]\n",
            "[[ 3.102]\n",
            " [18.4  ]\n",
            " [45.4  ]] 62.69 [38.56826]\n",
            "[[18.4 ]\n",
            " [45.4 ]\n",
            " [62.69]] 55.89 [55.03505]\n",
            "[[45.4 ]\n",
            " [62.69]\n",
            " [55.89]] 53.82 [34.745697]\n",
            "[[62.69]\n",
            " [55.89]\n",
            " [53.82]] 49.78 [60.21177]\n",
            "[[55.89]\n",
            " [53.82]\n",
            " [49.78]] 39.02 [41.141994]\n",
            "[[53.82]\n",
            " [49.78]\n",
            " [39.02]] 39.25 [22.331747]\n",
            "[[49.78]\n",
            " [39.02]\n",
            " [39.25]] 33.63 [47.08503]\n",
            "[[39.02]\n",
            " [39.25]\n",
            " [33.63]] 21.36 [20.625784]\n",
            "[[39.25]\n",
            " [33.63]\n",
            " [21.36]] 13.94 [11.424138]\n",
            "[[33.63]\n",
            " [21.36]\n",
            " [13.94]] 8.779 [7.934799]\n",
            "[[21.36 ]\n",
            " [13.94 ]\n",
            " [ 8.779]] 9.718 [5.1800675]\n",
            "[[13.94 ]\n",
            " [ 8.779]\n",
            " [ 9.718]] 10.73 [6.893275]\n",
            "[[ 8.779]\n",
            " [ 9.718]\n",
            " [10.73 ]] 9.286 [8.774528]\n",
            "[[ 9.718]\n",
            " [10.73 ]\n",
            " [ 9.286]] 7.464 [6.8838]\n",
            "[[10.73 ]\n",
            " [ 9.286]\n",
            " [ 7.464]] 6.291 [5.352201]\n",
            "[[9.286]\n",
            " [7.464]\n",
            " [6.291]] 5.438 [4.7049913]\n",
            "[[7.464]\n",
            " [6.291]\n",
            " [5.438]] 5.994 [4.272222]\n",
            "[[6.291]\n",
            " [5.438]\n",
            " [5.994]] 6.074 [4.907832]\n",
            "[[5.438]\n",
            " [5.994]\n",
            " [6.074]] 5.497 [4.784217]\n",
            "[[5.994]\n",
            " [6.074]\n",
            " [5.497]] 5.776 [4.261683]\n",
            "[[6.074]\n",
            " [5.497]\n",
            " [5.776]] 5.11 [4.681239]\n",
            "[[5.497]\n",
            " [5.776]\n",
            " [5.11 ]] 3.651 [3.9887362]\n",
            "[[5.776]\n",
            " [5.11 ]\n",
            " [3.651]] 3.636 [2.5259438]\n",
            "[[5.11 ]\n",
            " [3.651]\n",
            " [3.636]] 12.38 [3.2658772]\n",
            "[[ 3.651]\n",
            " [ 3.636]\n",
            " [12.38 ]] 15.68 [11.939524]\n",
            "[[ 3.636]\n",
            " [12.38 ]\n",
            " [15.68 ]] 10.12 [14.708987]\n",
            "[[12.38]\n",
            " [15.68]\n",
            " [10.12]] 7.821 [7.0523815]\n",
            "[[15.68 ]\n",
            " [10.12 ]\n",
            " [ 7.821]] 31.46 [4.971266]\n",
            "[[10.12 ]\n",
            " [ 7.821]\n",
            " [31.46 ]] 39.06 [28.771294]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Résumé du modèle"
      ],
      "metadata": {
        "id": "nkNfwRo_lIr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_nuYWmBlNDU",
        "outputId": "d07018a4-b18b-4940-cfc6-f7507aa212ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 150)               91200     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 50)                7550      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101351 (395.90 KB)\n",
            "Trainable params: 101351 (395.90 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 2: Série à input multiple: nous allons maintenant prédire les débits en fonction des débits passés, mais aussi en fonction des températures et précipitations passées (d'où le \"input multiple\")"
      ],
      "metadata": {
        "id": "gpJPixo6siIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Assignation des séries à des séquences spécifiques"
      ],
      "metadata": {
        "id": "b5aUxSNVt7v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_seq2 = np.array(df)\n",
        "raw_seq2 = raw_seq2[:,3:]\n",
        "raw_seq2 = raw_seq2.astype(np.float32)\n",
        "print(raw_seq2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_Sdd8EluDlx",
        "outputId": "551b93a9-6464-4459-ef15-fbb2e15db3e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 7.2    0.    17.41 ]\n",
            " [ 6.4    0.    25.37 ]\n",
            " [ 5.7    0.    30.38 ]\n",
            " [ 7.6    0.    36.74 ]\n",
            " [ 5.9   11.8   51.5  ]\n",
            " [ 5.2    0.7   69.64 ]\n",
            " [ 4.9    1.7   72.08 ]\n",
            " [ 2.6    0.    57.16 ]\n",
            " [ 3.5    0.    42.24 ]\n",
            " [10.2    0.    34.46 ]\n",
            " [ 8.9    9.3   37.96 ]\n",
            " [ 5.1    8.9   48.31 ]\n",
            " [ 6.2    3.1   58.88 ]\n",
            " [ 8.7    3.1   53.85 ]\n",
            " [ 6.4    0.    45.15 ]\n",
            " [ 8.5    0.    37.22 ]\n",
            " [ 9.2    0.2   31.66 ]\n",
            " [ 9.    16.7   32.68 ]\n",
            " [ 7.2   24.6   42.2  ]\n",
            " [ 9.2    0.    63.83 ]\n",
            " [ 6.5    0.2   55.17 ]\n",
            " [ 8.4    1.4   38.87 ]\n",
            " [12.4    0.    24.79 ]\n",
            " [15.1    0.    17.85 ]\n",
            " [13.     0.    15.75 ]\n",
            " [ 7.4    0.    13.19 ]\n",
            " [ 7.     0.     9.438]\n",
            " [12.3    0.     8.42 ]\n",
            " [13.2    0.     6.156]\n",
            " [16.2    0.     5.106]\n",
            " [ 9.1    0.     6.626]\n",
            " [11.4    0.     5.63 ]\n",
            " [12.9    0.     4.519]\n",
            " [ 5.3    3.1    4.226]\n",
            " [ 3.2    0.     4.337]\n",
            " [ 5.6    0.     3.58 ]\n",
            " [11.     0.     3.411]\n",
            " [17.1    4.8    3.469]\n",
            " [12.    18.3    6.423]\n",
            " [ 6.7    0.     7.25 ]\n",
            " [11.7    0.     5.621]\n",
            " [10.2    0.7    4.085]\n",
            " [10.     0.     3.882]\n",
            " [11.6    0.     3.391]\n",
            " [15.8    0.     3.082]\n",
            " [20.6    0.     2.755]\n",
            " [12.3    0.     2.36 ]\n",
            " [15.3    0.     2.055]\n",
            " [18.6    0.     1.831]\n",
            " [23.1   11.3    1.961]\n",
            " [14.1    0.     2.449]\n",
            " [10.1    0.     2.189]\n",
            " [10.3    0.     1.857]\n",
            " [11.8    0.     1.59 ]\n",
            " [12.7    3.     1.617]\n",
            " [11.7    4.9    1.908]\n",
            " [12.4    0.     1.93 ]\n",
            " [13.2    0.5    1.987]\n",
            " [14.6    0.7    1.525]\n",
            " [17.7    0.     1.478]\n",
            " [19.3    0.     1.261]\n",
            " [16.2   14.5    1.832]\n",
            " [17.4    4.4    2.562]\n",
            " [14.1    8.1    3.053]\n",
            " [14.5    0.     3.262]\n",
            " [13.1   14.1    3.716]\n",
            " [13.4    1.7    3.651]\n",
            " [14.5    2.7    3.067]\n",
            " [15.9    0.2    2.844]\n",
            " [17.9    0.     2.336]\n",
            " [18.3    0.     1.84 ]\n",
            " [19.5    0.     1.528]\n",
            " [20.6    0.     1.514]\n",
            " [20.5    0.     1.554]\n",
            " [17.1    2.1    1.146]\n",
            " [19.6    3.7    1.404]\n",
            " [22.2   10.8    2.057]\n",
            " [17.5    3.     6.54 ]\n",
            " [20.3    0.    10.23 ]\n",
            " [21.5    0.4    7.732]\n",
            " [21.8   19.5   11.44 ]\n",
            " [20.8    0.    13.41 ]\n",
            " [23.3    0.     9.968]\n",
            " [23.9    0.     5.758]\n",
            " [24.8    0.     3.485]\n",
            " [22.6   14.5    2.562]\n",
            " [22.6    0.     2.538]\n",
            " [23.4    7.3    3.102]\n",
            " [20.    47.4   18.4  ]\n",
            " [20.    30.9   45.4  ]\n",
            " [20.3    0.    62.69 ]\n",
            " [18.3   58.1   55.89 ]\n",
            " [21.3    0.    53.82 ]\n",
            " [22.8    0.    49.78 ]\n",
            " [20.1   24.4   39.02 ]\n",
            " [22.2    0.1   39.25 ]\n",
            " [20.3    2.4   33.63 ]\n",
            " [20.1    1.1   21.36 ]\n",
            " [20.5    0.    13.94 ]\n",
            " [18.     6.5    8.779]\n",
            " [19.5    6.6    9.718]\n",
            " [20.2    0.    10.73 ]\n",
            " [20.3    0.     9.286]\n",
            " [20.5    0.     7.464]\n",
            " [19.85   0.     6.291]\n",
            " [22.6    1.     5.438]\n",
            " [21.8   35.7    5.994]\n",
            " [16.     0.     6.074]\n",
            " [16.     0.     5.497]\n",
            " [16.1    8.8    5.776]\n",
            " [15.2    0.     5.11 ]\n",
            " [14.7    0.     3.651]\n",
            " [15.     9.2    3.636]\n",
            " [17.6   10.6   12.38 ]\n",
            " [17.5    0.    15.68 ]\n",
            " [16.7    0.    10.12 ]\n",
            " [16.5    3.9    7.821]\n",
            " [16.5   77.9   31.46 ]\n",
            " [17.5    0.    39.06 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Fonction servant à formatter les données d'une séquence avec input multiple"
      ],
      "metadata": {
        "id": "ELtdxSIyvrdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sequence2(sequence, n_steps):\n",
        " X, y = list(), list()\n",
        " for i in range(len(sequence)):\n",
        " # find the end of this pattern\n",
        "  end_ix = i + n_steps\n",
        " # check if we are beyond the sequence\n",
        "  if end_ix > len(sequence)-1:\n",
        "    break\n",
        " # gather input and output parts of the pattern\n",
        "  seq_x, seq_y = sequence[i:end_ix,:], sequence[end_ix,-1]\n",
        "  X.append(seq_x)\n",
        "  y.append(seq_y)\n",
        " return array(X), array(y)"
      ],
      "metadata": {
        "id": "mcBRZxa2vwJK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Formatter les données à l'aide de la fonction précédente"
      ],
      "metadata": {
        "id": "ZN2Q0Mm-wz7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 3\n",
        "\n",
        "X, y = split_sequence2(raw_seq2, n_steps)\n",
        "\n",
        "n_features = 3\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "for i in range(len(X)):\n",
        " print(X[i], y[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvOLZ4zaw3Oc",
        "outputId": "39c80089-bc2c-4e1b-a268-27d48bade386"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(116, 3, 3) (116,)\n",
            "[[ 7.2   0.   17.41]\n",
            " [ 6.4   0.   25.37]\n",
            " [ 5.7   0.   30.38]] 36.74\n",
            "[[ 6.4   0.   25.37]\n",
            " [ 5.7   0.   30.38]\n",
            " [ 7.6   0.   36.74]] 51.5\n",
            "[[ 5.7   0.   30.38]\n",
            " [ 7.6   0.   36.74]\n",
            " [ 5.9  11.8  51.5 ]] 69.64\n",
            "[[ 7.6   0.   36.74]\n",
            " [ 5.9  11.8  51.5 ]\n",
            " [ 5.2   0.7  69.64]] 72.08\n",
            "[[ 5.9  11.8  51.5 ]\n",
            " [ 5.2   0.7  69.64]\n",
            " [ 4.9   1.7  72.08]] 57.16\n",
            "[[ 5.2   0.7  69.64]\n",
            " [ 4.9   1.7  72.08]\n",
            " [ 2.6   0.   57.16]] 42.24\n",
            "[[ 4.9   1.7  72.08]\n",
            " [ 2.6   0.   57.16]\n",
            " [ 3.5   0.   42.24]] 34.46\n",
            "[[ 2.6   0.   57.16]\n",
            " [ 3.5   0.   42.24]\n",
            " [10.2   0.   34.46]] 37.96\n",
            "[[ 3.5   0.   42.24]\n",
            " [10.2   0.   34.46]\n",
            " [ 8.9   9.3  37.96]] 48.31\n",
            "[[10.2   0.   34.46]\n",
            " [ 8.9   9.3  37.96]\n",
            " [ 5.1   8.9  48.31]] 58.88\n",
            "[[ 8.9   9.3  37.96]\n",
            " [ 5.1   8.9  48.31]\n",
            " [ 6.2   3.1  58.88]] 53.85\n",
            "[[ 5.1   8.9  48.31]\n",
            " [ 6.2   3.1  58.88]\n",
            " [ 8.7   3.1  53.85]] 45.15\n",
            "[[ 6.2   3.1  58.88]\n",
            " [ 8.7   3.1  53.85]\n",
            " [ 6.4   0.   45.15]] 37.22\n",
            "[[ 8.7   3.1  53.85]\n",
            " [ 6.4   0.   45.15]\n",
            " [ 8.5   0.   37.22]] 31.66\n",
            "[[ 6.4   0.   45.15]\n",
            " [ 8.5   0.   37.22]\n",
            " [ 9.2   0.2  31.66]] 32.68\n",
            "[[ 8.5   0.   37.22]\n",
            " [ 9.2   0.2  31.66]\n",
            " [ 9.   16.7  32.68]] 42.2\n",
            "[[ 9.2   0.2  31.66]\n",
            " [ 9.   16.7  32.68]\n",
            " [ 7.2  24.6  42.2 ]] 63.83\n",
            "[[ 9.   16.7  32.68]\n",
            " [ 7.2  24.6  42.2 ]\n",
            " [ 9.2   0.   63.83]] 55.17\n",
            "[[ 7.2  24.6  42.2 ]\n",
            " [ 9.2   0.   63.83]\n",
            " [ 6.5   0.2  55.17]] 38.87\n",
            "[[ 9.2   0.   63.83]\n",
            " [ 6.5   0.2  55.17]\n",
            " [ 8.4   1.4  38.87]] 24.79\n",
            "[[ 6.5   0.2  55.17]\n",
            " [ 8.4   1.4  38.87]\n",
            " [12.4   0.   24.79]] 17.85\n",
            "[[ 8.4   1.4  38.87]\n",
            " [12.4   0.   24.79]\n",
            " [15.1   0.   17.85]] 15.75\n",
            "[[12.4   0.   24.79]\n",
            " [15.1   0.   17.85]\n",
            " [13.    0.   15.75]] 13.19\n",
            "[[15.1   0.   17.85]\n",
            " [13.    0.   15.75]\n",
            " [ 7.4   0.   13.19]] 9.438\n",
            "[[13.     0.    15.75 ]\n",
            " [ 7.4    0.    13.19 ]\n",
            " [ 7.     0.     9.438]] 8.42\n",
            "[[ 7.4    0.    13.19 ]\n",
            " [ 7.     0.     9.438]\n",
            " [12.3    0.     8.42 ]] 6.156\n",
            "[[ 7.     0.     9.438]\n",
            " [12.3    0.     8.42 ]\n",
            " [13.2    0.     6.156]] 5.106\n",
            "[[12.3    0.     8.42 ]\n",
            " [13.2    0.     6.156]\n",
            " [16.2    0.     5.106]] 6.626\n",
            "[[13.2    0.     6.156]\n",
            " [16.2    0.     5.106]\n",
            " [ 9.1    0.     6.626]] 5.63\n",
            "[[16.2    0.     5.106]\n",
            " [ 9.1    0.     6.626]\n",
            " [11.4    0.     5.63 ]] 4.519\n",
            "[[ 9.1    0.     6.626]\n",
            " [11.4    0.     5.63 ]\n",
            " [12.9    0.     4.519]] 4.226\n",
            "[[11.4    0.     5.63 ]\n",
            " [12.9    0.     4.519]\n",
            " [ 5.3    3.1    4.226]] 4.337\n",
            "[[12.9    0.     4.519]\n",
            " [ 5.3    3.1    4.226]\n",
            " [ 3.2    0.     4.337]] 3.58\n",
            "[[5.3   3.1   4.226]\n",
            " [3.2   0.    4.337]\n",
            " [5.6   0.    3.58 ]] 3.411\n",
            "[[ 3.2    0.     4.337]\n",
            " [ 5.6    0.     3.58 ]\n",
            " [11.     0.     3.411]] 3.469\n",
            "[[ 5.6    0.     3.58 ]\n",
            " [11.     0.     3.411]\n",
            " [17.1    4.8    3.469]] 6.423\n",
            "[[11.     0.     3.411]\n",
            " [17.1    4.8    3.469]\n",
            " [12.    18.3    6.423]] 7.25\n",
            "[[17.1    4.8    3.469]\n",
            " [12.    18.3    6.423]\n",
            " [ 6.7    0.     7.25 ]] 5.621\n",
            "[[12.    18.3    6.423]\n",
            " [ 6.7    0.     7.25 ]\n",
            " [11.7    0.     5.621]] 4.085\n",
            "[[ 6.7    0.     7.25 ]\n",
            " [11.7    0.     5.621]\n",
            " [10.2    0.7    4.085]] 3.882\n",
            "[[11.7    0.     5.621]\n",
            " [10.2    0.7    4.085]\n",
            " [10.     0.     3.882]] 3.391\n",
            "[[10.2    0.7    4.085]\n",
            " [10.     0.     3.882]\n",
            " [11.6    0.     3.391]] 3.082\n",
            "[[10.     0.     3.882]\n",
            " [11.6    0.     3.391]\n",
            " [15.8    0.     3.082]] 2.755\n",
            "[[11.6    0.     3.391]\n",
            " [15.8    0.     3.082]\n",
            " [20.6    0.     2.755]] 2.36\n",
            "[[15.8    0.     3.082]\n",
            " [20.6    0.     2.755]\n",
            " [12.3    0.     2.36 ]] 2.055\n",
            "[[20.6    0.     2.755]\n",
            " [12.3    0.     2.36 ]\n",
            " [15.3    0.     2.055]] 1.831\n",
            "[[12.3    0.     2.36 ]\n",
            " [15.3    0.     2.055]\n",
            " [18.6    0.     1.831]] 1.961\n",
            "[[15.3    0.     2.055]\n",
            " [18.6    0.     1.831]\n",
            " [23.1   11.3    1.961]] 2.449\n",
            "[[18.6    0.     1.831]\n",
            " [23.1   11.3    1.961]\n",
            " [14.1    0.     2.449]] 2.189\n",
            "[[23.1   11.3    1.961]\n",
            " [14.1    0.     2.449]\n",
            " [10.1    0.     2.189]] 1.857\n",
            "[[14.1    0.     2.449]\n",
            " [10.1    0.     2.189]\n",
            " [10.3    0.     1.857]] 1.59\n",
            "[[10.1    0.     2.189]\n",
            " [10.3    0.     1.857]\n",
            " [11.8    0.     1.59 ]] 1.617\n",
            "[[10.3    0.     1.857]\n",
            " [11.8    0.     1.59 ]\n",
            " [12.7    3.     1.617]] 1.908\n",
            "[[11.8    0.     1.59 ]\n",
            " [12.7    3.     1.617]\n",
            " [11.7    4.9    1.908]] 1.93\n",
            "[[12.7    3.     1.617]\n",
            " [11.7    4.9    1.908]\n",
            " [12.4    0.     1.93 ]] 1.987\n",
            "[[11.7    4.9    1.908]\n",
            " [12.4    0.     1.93 ]\n",
            " [13.2    0.5    1.987]] 1.525\n",
            "[[12.4    0.     1.93 ]\n",
            " [13.2    0.5    1.987]\n",
            " [14.6    0.7    1.525]] 1.478\n",
            "[[13.2    0.5    1.987]\n",
            " [14.6    0.7    1.525]\n",
            " [17.7    0.     1.478]] 1.261\n",
            "[[14.6    0.7    1.525]\n",
            " [17.7    0.     1.478]\n",
            " [19.3    0.     1.261]] 1.832\n",
            "[[17.7    0.     1.478]\n",
            " [19.3    0.     1.261]\n",
            " [16.2   14.5    1.832]] 2.562\n",
            "[[19.3    0.     1.261]\n",
            " [16.2   14.5    1.832]\n",
            " [17.4    4.4    2.562]] 3.053\n",
            "[[16.2   14.5    1.832]\n",
            " [17.4    4.4    2.562]\n",
            " [14.1    8.1    3.053]] 3.262\n",
            "[[17.4    4.4    2.562]\n",
            " [14.1    8.1    3.053]\n",
            " [14.5    0.     3.262]] 3.716\n",
            "[[14.1    8.1    3.053]\n",
            " [14.5    0.     3.262]\n",
            " [13.1   14.1    3.716]] 3.651\n",
            "[[14.5    0.     3.262]\n",
            " [13.1   14.1    3.716]\n",
            " [13.4    1.7    3.651]] 3.067\n",
            "[[13.1   14.1    3.716]\n",
            " [13.4    1.7    3.651]\n",
            " [14.5    2.7    3.067]] 2.844\n",
            "[[13.4    1.7    3.651]\n",
            " [14.5    2.7    3.067]\n",
            " [15.9    0.2    2.844]] 2.336\n",
            "[[14.5    2.7    3.067]\n",
            " [15.9    0.2    2.844]\n",
            " [17.9    0.     2.336]] 1.84\n",
            "[[15.9    0.2    2.844]\n",
            " [17.9    0.     2.336]\n",
            " [18.3    0.     1.84 ]] 1.528\n",
            "[[17.9    0.     2.336]\n",
            " [18.3    0.     1.84 ]\n",
            " [19.5    0.     1.528]] 1.514\n",
            "[[18.3    0.     1.84 ]\n",
            " [19.5    0.     1.528]\n",
            " [20.6    0.     1.514]] 1.554\n",
            "[[19.5    0.     1.528]\n",
            " [20.6    0.     1.514]\n",
            " [20.5    0.     1.554]] 1.146\n",
            "[[20.6    0.     1.514]\n",
            " [20.5    0.     1.554]\n",
            " [17.1    2.1    1.146]] 1.404\n",
            "[[20.5    0.     1.554]\n",
            " [17.1    2.1    1.146]\n",
            " [19.6    3.7    1.404]] 2.057\n",
            "[[17.1    2.1    1.146]\n",
            " [19.6    3.7    1.404]\n",
            " [22.2   10.8    2.057]] 6.54\n",
            "[[19.6    3.7    1.404]\n",
            " [22.2   10.8    2.057]\n",
            " [17.5    3.     6.54 ]] 10.23\n",
            "[[22.2   10.8    2.057]\n",
            " [17.5    3.     6.54 ]\n",
            " [20.3    0.    10.23 ]] 7.732\n",
            "[[17.5    3.     6.54 ]\n",
            " [20.3    0.    10.23 ]\n",
            " [21.5    0.4    7.732]] 11.44\n",
            "[[20.3    0.    10.23 ]\n",
            " [21.5    0.4    7.732]\n",
            " [21.8   19.5   11.44 ]] 13.41\n",
            "[[21.5    0.4    7.732]\n",
            " [21.8   19.5   11.44 ]\n",
            " [20.8    0.    13.41 ]] 9.968\n",
            "[[21.8   19.5   11.44 ]\n",
            " [20.8    0.    13.41 ]\n",
            " [23.3    0.     9.968]] 5.758\n",
            "[[20.8    0.    13.41 ]\n",
            " [23.3    0.     9.968]\n",
            " [23.9    0.     5.758]] 3.485\n",
            "[[23.3    0.     9.968]\n",
            " [23.9    0.     5.758]\n",
            " [24.8    0.     3.485]] 2.562\n",
            "[[23.9    0.     5.758]\n",
            " [24.8    0.     3.485]\n",
            " [22.6   14.5    2.562]] 2.538\n",
            "[[24.8    0.     3.485]\n",
            " [22.6   14.5    2.562]\n",
            " [22.6    0.     2.538]] 3.102\n",
            "[[22.6   14.5    2.562]\n",
            " [22.6    0.     2.538]\n",
            " [23.4    7.3    3.102]] 18.4\n",
            "[[22.6    0.     2.538]\n",
            " [23.4    7.3    3.102]\n",
            " [20.    47.4   18.4  ]] 45.4\n",
            "[[23.4    7.3    3.102]\n",
            " [20.    47.4   18.4  ]\n",
            " [20.    30.9   45.4  ]] 62.69\n",
            "[[20.   47.4  18.4 ]\n",
            " [20.   30.9  45.4 ]\n",
            " [20.3   0.   62.69]] 55.89\n",
            "[[20.   30.9  45.4 ]\n",
            " [20.3   0.   62.69]\n",
            " [18.3  58.1  55.89]] 53.82\n",
            "[[20.3   0.   62.69]\n",
            " [18.3  58.1  55.89]\n",
            " [21.3   0.   53.82]] 49.78\n",
            "[[18.3  58.1  55.89]\n",
            " [21.3   0.   53.82]\n",
            " [22.8   0.   49.78]] 39.02\n",
            "[[21.3   0.   53.82]\n",
            " [22.8   0.   49.78]\n",
            " [20.1  24.4  39.02]] 39.25\n",
            "[[22.8   0.   49.78]\n",
            " [20.1  24.4  39.02]\n",
            " [22.2   0.1  39.25]] 33.63\n",
            "[[20.1  24.4  39.02]\n",
            " [22.2   0.1  39.25]\n",
            " [20.3   2.4  33.63]] 21.36\n",
            "[[22.2   0.1  39.25]\n",
            " [20.3   2.4  33.63]\n",
            " [20.1   1.1  21.36]] 13.94\n",
            "[[20.3   2.4  33.63]\n",
            " [20.1   1.1  21.36]\n",
            " [20.5   0.   13.94]] 8.779\n",
            "[[20.1    1.1   21.36 ]\n",
            " [20.5    0.    13.94 ]\n",
            " [18.     6.5    8.779]] 9.718\n",
            "[[20.5    0.    13.94 ]\n",
            " [18.     6.5    8.779]\n",
            " [19.5    6.6    9.718]] 10.73\n",
            "[[18.     6.5    8.779]\n",
            " [19.5    6.6    9.718]\n",
            " [20.2    0.    10.73 ]] 9.286\n",
            "[[19.5    6.6    9.718]\n",
            " [20.2    0.    10.73 ]\n",
            " [20.3    0.     9.286]] 7.464\n",
            "[[20.2    0.    10.73 ]\n",
            " [20.3    0.     9.286]\n",
            " [20.5    0.     7.464]] 6.291\n",
            "[[20.3    0.     9.286]\n",
            " [20.5    0.     7.464]\n",
            " [19.85   0.     6.291]] 5.438\n",
            "[[20.5    0.     7.464]\n",
            " [19.85   0.     6.291]\n",
            " [22.6    1.     5.438]] 5.994\n",
            "[[19.85   0.     6.291]\n",
            " [22.6    1.     5.438]\n",
            " [21.8   35.7    5.994]] 6.074\n",
            "[[22.6    1.     5.438]\n",
            " [21.8   35.7    5.994]\n",
            " [16.     0.     6.074]] 5.497\n",
            "[[21.8   35.7    5.994]\n",
            " [16.     0.     6.074]\n",
            " [16.     0.     5.497]] 5.776\n",
            "[[16.     0.     6.074]\n",
            " [16.     0.     5.497]\n",
            " [16.1    8.8    5.776]] 5.11\n",
            "[[16.     0.     5.497]\n",
            " [16.1    8.8    5.776]\n",
            " [15.2    0.     5.11 ]] 3.651\n",
            "[[16.1    8.8    5.776]\n",
            " [15.2    0.     5.11 ]\n",
            " [14.7    0.     3.651]] 3.636\n",
            "[[15.2    0.     5.11 ]\n",
            " [14.7    0.     3.651]\n",
            " [15.     9.2    3.636]] 12.38\n",
            "[[14.7    0.     3.651]\n",
            " [15.     9.2    3.636]\n",
            " [17.6   10.6   12.38 ]] 15.68\n",
            "[[15.     9.2    3.636]\n",
            " [17.6   10.6   12.38 ]\n",
            " [17.5    0.    15.68 ]] 10.12\n",
            "[[17.6  10.6  12.38]\n",
            " [17.5   0.   15.68]\n",
            " [16.7   0.   10.12]] 7.821\n",
            "[[17.5    0.    15.68 ]\n",
            " [16.7    0.    10.12 ]\n",
            " [16.5    3.9    7.821]] 31.46\n",
            "[[16.7    0.    10.12 ]\n",
            " [16.5    3.9    7.821]\n",
            " [16.5   77.9   31.46 ]] 39.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Diviser les données en train, valid et test data"
      ],
      "metadata": {
        "id": "JBuwMeayzJ1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = X[:70], y[:70]\n",
        "X_valid, y_valid = X[70:80], y[70:80]\n",
        "X_test, y_test = X[80:], y[80:]\n",
        "\n",
        "print(X_train.shape, X_valid.shape, X_test.shape)\n",
        "\n",
        "for i in range(len(X_train)):\n",
        " print(X_train[i], y_train[i])\n",
        "\n",
        "#for i in range(len(X_test)):\n",
        "# print(X_test[i], y_test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35hXDtbbzLsk",
        "outputId": "e9d3503a-8d60-4155-9199-cc5e76c62523"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 3, 3) (10, 3, 3) (36, 3, 3)\n",
            "[[ 7.2   0.   17.41]\n",
            " [ 6.4   0.   25.37]\n",
            " [ 5.7   0.   30.38]] 36.74\n",
            "[[ 6.4   0.   25.37]\n",
            " [ 5.7   0.   30.38]\n",
            " [ 7.6   0.   36.74]] 51.5\n",
            "[[ 5.7   0.   30.38]\n",
            " [ 7.6   0.   36.74]\n",
            " [ 5.9  11.8  51.5 ]] 69.64\n",
            "[[ 7.6   0.   36.74]\n",
            " [ 5.9  11.8  51.5 ]\n",
            " [ 5.2   0.7  69.64]] 72.08\n",
            "[[ 5.9  11.8  51.5 ]\n",
            " [ 5.2   0.7  69.64]\n",
            " [ 4.9   1.7  72.08]] 57.16\n",
            "[[ 5.2   0.7  69.64]\n",
            " [ 4.9   1.7  72.08]\n",
            " [ 2.6   0.   57.16]] 42.24\n",
            "[[ 4.9   1.7  72.08]\n",
            " [ 2.6   0.   57.16]\n",
            " [ 3.5   0.   42.24]] 34.46\n",
            "[[ 2.6   0.   57.16]\n",
            " [ 3.5   0.   42.24]\n",
            " [10.2   0.   34.46]] 37.96\n",
            "[[ 3.5   0.   42.24]\n",
            " [10.2   0.   34.46]\n",
            " [ 8.9   9.3  37.96]] 48.31\n",
            "[[10.2   0.   34.46]\n",
            " [ 8.9   9.3  37.96]\n",
            " [ 5.1   8.9  48.31]] 58.88\n",
            "[[ 8.9   9.3  37.96]\n",
            " [ 5.1   8.9  48.31]\n",
            " [ 6.2   3.1  58.88]] 53.85\n",
            "[[ 5.1   8.9  48.31]\n",
            " [ 6.2   3.1  58.88]\n",
            " [ 8.7   3.1  53.85]] 45.15\n",
            "[[ 6.2   3.1  58.88]\n",
            " [ 8.7   3.1  53.85]\n",
            " [ 6.4   0.   45.15]] 37.22\n",
            "[[ 8.7   3.1  53.85]\n",
            " [ 6.4   0.   45.15]\n",
            " [ 8.5   0.   37.22]] 31.66\n",
            "[[ 6.4   0.   45.15]\n",
            " [ 8.5   0.   37.22]\n",
            " [ 9.2   0.2  31.66]] 32.68\n",
            "[[ 8.5   0.   37.22]\n",
            " [ 9.2   0.2  31.66]\n",
            " [ 9.   16.7  32.68]] 42.2\n",
            "[[ 9.2   0.2  31.66]\n",
            " [ 9.   16.7  32.68]\n",
            " [ 7.2  24.6  42.2 ]] 63.83\n",
            "[[ 9.   16.7  32.68]\n",
            " [ 7.2  24.6  42.2 ]\n",
            " [ 9.2   0.   63.83]] 55.17\n",
            "[[ 7.2  24.6  42.2 ]\n",
            " [ 9.2   0.   63.83]\n",
            " [ 6.5   0.2  55.17]] 38.87\n",
            "[[ 9.2   0.   63.83]\n",
            " [ 6.5   0.2  55.17]\n",
            " [ 8.4   1.4  38.87]] 24.79\n",
            "[[ 6.5   0.2  55.17]\n",
            " [ 8.4   1.4  38.87]\n",
            " [12.4   0.   24.79]] 17.85\n",
            "[[ 8.4   1.4  38.87]\n",
            " [12.4   0.   24.79]\n",
            " [15.1   0.   17.85]] 15.75\n",
            "[[12.4   0.   24.79]\n",
            " [15.1   0.   17.85]\n",
            " [13.    0.   15.75]] 13.19\n",
            "[[15.1   0.   17.85]\n",
            " [13.    0.   15.75]\n",
            " [ 7.4   0.   13.19]] 9.438\n",
            "[[13.     0.    15.75 ]\n",
            " [ 7.4    0.    13.19 ]\n",
            " [ 7.     0.     9.438]] 8.42\n",
            "[[ 7.4    0.    13.19 ]\n",
            " [ 7.     0.     9.438]\n",
            " [12.3    0.     8.42 ]] 6.156\n",
            "[[ 7.     0.     9.438]\n",
            " [12.3    0.     8.42 ]\n",
            " [13.2    0.     6.156]] 5.106\n",
            "[[12.3    0.     8.42 ]\n",
            " [13.2    0.     6.156]\n",
            " [16.2    0.     5.106]] 6.626\n",
            "[[13.2    0.     6.156]\n",
            " [16.2    0.     5.106]\n",
            " [ 9.1    0.     6.626]] 5.63\n",
            "[[16.2    0.     5.106]\n",
            " [ 9.1    0.     6.626]\n",
            " [11.4    0.     5.63 ]] 4.519\n",
            "[[ 9.1    0.     6.626]\n",
            " [11.4    0.     5.63 ]\n",
            " [12.9    0.     4.519]] 4.226\n",
            "[[11.4    0.     5.63 ]\n",
            " [12.9    0.     4.519]\n",
            " [ 5.3    3.1    4.226]] 4.337\n",
            "[[12.9    0.     4.519]\n",
            " [ 5.3    3.1    4.226]\n",
            " [ 3.2    0.     4.337]] 3.58\n",
            "[[5.3   3.1   4.226]\n",
            " [3.2   0.    4.337]\n",
            " [5.6   0.    3.58 ]] 3.411\n",
            "[[ 3.2    0.     4.337]\n",
            " [ 5.6    0.     3.58 ]\n",
            " [11.     0.     3.411]] 3.469\n",
            "[[ 5.6    0.     3.58 ]\n",
            " [11.     0.     3.411]\n",
            " [17.1    4.8    3.469]] 6.423\n",
            "[[11.     0.     3.411]\n",
            " [17.1    4.8    3.469]\n",
            " [12.    18.3    6.423]] 7.25\n",
            "[[17.1    4.8    3.469]\n",
            " [12.    18.3    6.423]\n",
            " [ 6.7    0.     7.25 ]] 5.621\n",
            "[[12.    18.3    6.423]\n",
            " [ 6.7    0.     7.25 ]\n",
            " [11.7    0.     5.621]] 4.085\n",
            "[[ 6.7    0.     7.25 ]\n",
            " [11.7    0.     5.621]\n",
            " [10.2    0.7    4.085]] 3.882\n",
            "[[11.7    0.     5.621]\n",
            " [10.2    0.7    4.085]\n",
            " [10.     0.     3.882]] 3.391\n",
            "[[10.2    0.7    4.085]\n",
            " [10.     0.     3.882]\n",
            " [11.6    0.     3.391]] 3.082\n",
            "[[10.     0.     3.882]\n",
            " [11.6    0.     3.391]\n",
            " [15.8    0.     3.082]] 2.755\n",
            "[[11.6    0.     3.391]\n",
            " [15.8    0.     3.082]\n",
            " [20.6    0.     2.755]] 2.36\n",
            "[[15.8    0.     3.082]\n",
            " [20.6    0.     2.755]\n",
            " [12.3    0.     2.36 ]] 2.055\n",
            "[[20.6    0.     2.755]\n",
            " [12.3    0.     2.36 ]\n",
            " [15.3    0.     2.055]] 1.831\n",
            "[[12.3    0.     2.36 ]\n",
            " [15.3    0.     2.055]\n",
            " [18.6    0.     1.831]] 1.961\n",
            "[[15.3    0.     2.055]\n",
            " [18.6    0.     1.831]\n",
            " [23.1   11.3    1.961]] 2.449\n",
            "[[18.6    0.     1.831]\n",
            " [23.1   11.3    1.961]\n",
            " [14.1    0.     2.449]] 2.189\n",
            "[[23.1   11.3    1.961]\n",
            " [14.1    0.     2.449]\n",
            " [10.1    0.     2.189]] 1.857\n",
            "[[14.1    0.     2.449]\n",
            " [10.1    0.     2.189]\n",
            " [10.3    0.     1.857]] 1.59\n",
            "[[10.1    0.     2.189]\n",
            " [10.3    0.     1.857]\n",
            " [11.8    0.     1.59 ]] 1.617\n",
            "[[10.3    0.     1.857]\n",
            " [11.8    0.     1.59 ]\n",
            " [12.7    3.     1.617]] 1.908\n",
            "[[11.8    0.     1.59 ]\n",
            " [12.7    3.     1.617]\n",
            " [11.7    4.9    1.908]] 1.93\n",
            "[[12.7    3.     1.617]\n",
            " [11.7    4.9    1.908]\n",
            " [12.4    0.     1.93 ]] 1.987\n",
            "[[11.7    4.9    1.908]\n",
            " [12.4    0.     1.93 ]\n",
            " [13.2    0.5    1.987]] 1.525\n",
            "[[12.4    0.     1.93 ]\n",
            " [13.2    0.5    1.987]\n",
            " [14.6    0.7    1.525]] 1.478\n",
            "[[13.2    0.5    1.987]\n",
            " [14.6    0.7    1.525]\n",
            " [17.7    0.     1.478]] 1.261\n",
            "[[14.6    0.7    1.525]\n",
            " [17.7    0.     1.478]\n",
            " [19.3    0.     1.261]] 1.832\n",
            "[[17.7    0.     1.478]\n",
            " [19.3    0.     1.261]\n",
            " [16.2   14.5    1.832]] 2.562\n",
            "[[19.3    0.     1.261]\n",
            " [16.2   14.5    1.832]\n",
            " [17.4    4.4    2.562]] 3.053\n",
            "[[16.2   14.5    1.832]\n",
            " [17.4    4.4    2.562]\n",
            " [14.1    8.1    3.053]] 3.262\n",
            "[[17.4    4.4    2.562]\n",
            " [14.1    8.1    3.053]\n",
            " [14.5    0.     3.262]] 3.716\n",
            "[[14.1    8.1    3.053]\n",
            " [14.5    0.     3.262]\n",
            " [13.1   14.1    3.716]] 3.651\n",
            "[[14.5    0.     3.262]\n",
            " [13.1   14.1    3.716]\n",
            " [13.4    1.7    3.651]] 3.067\n",
            "[[13.1   14.1    3.716]\n",
            " [13.4    1.7    3.651]\n",
            " [14.5    2.7    3.067]] 2.844\n",
            "[[13.4    1.7    3.651]\n",
            " [14.5    2.7    3.067]\n",
            " [15.9    0.2    2.844]] 2.336\n",
            "[[14.5    2.7    3.067]\n",
            " [15.9    0.2    2.844]\n",
            " [17.9    0.     2.336]] 1.84\n",
            "[[15.9    0.2    2.844]\n",
            " [17.9    0.     2.336]\n",
            " [18.3    0.     1.84 ]] 1.528\n",
            "[[17.9    0.     2.336]\n",
            " [18.3    0.     1.84 ]\n",
            " [19.5    0.     1.528]] 1.514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Définir le modèle LSTM à input multiple"
      ],
      "metadata": {
        "id": "MtdS4qchzeem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(150, activation='relu', kernel_initializer=\"he_normal\",input_shape=(n_steps, n_features)))\n",
        "model.add(Dense(50,activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50,activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "ILmSYDZRziA2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Entrainer le modèle"
      ],
      "metadata": {
        "id": "ZW9o1fsVzr-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=1, validation_data=[X_valid,y_valid])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAQXXEEMztrC",
        "outputId": "8c71737d-7aac-41f7-a312-1768212be03f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3/3 [==============================] - 2s 170ms/step - loss: 5593.2090 - val_loss: 784.9611\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 2142.7742 - val_loss: 238.0730\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1243.1100 - val_loss: 51.6351\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 674.9398 - val_loss: 25.8151\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 230.7498 - val_loss: 62.3183\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 462.5409 - val_loss: 88.1824\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 275.0625 - val_loss: 81.8810\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 316.8456 - val_loss: 71.9619\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 364.7011 - val_loss: 67.4382\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 191.4506 - val_loss: 61.1429\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 178.2813 - val_loss: 51.1716\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 196.5036 - val_loss: 41.0477\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 182.2880 - val_loss: 31.9417\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 152.0003 - val_loss: 26.2445\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 214.9787 - val_loss: 23.5390\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 145.5652 - val_loss: 22.3215\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 182.5342 - val_loss: 21.4395\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 135.9827 - val_loss: 19.6293\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 129.8474 - val_loss: 18.2948\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 224.4633 - val_loss: 18.0400\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 119.6353 - val_loss: 18.3399\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 149.6924 - val_loss: 18.1948\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 98.5146 - val_loss: 18.0241\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 101.3405 - val_loss: 17.6049\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 131.1108 - val_loss: 16.9663\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 149.5910 - val_loss: 15.7904\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 140.6682 - val_loss: 14.1404\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 70.1852 - val_loss: 13.0330\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 63.5996 - val_loss: 12.5907\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 66.1809 - val_loss: 12.4709\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 107.9947 - val_loss: 12.1074\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 75.4096 - val_loss: 11.8922\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 86.9450 - val_loss: 11.8689\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 107.5721 - val_loss: 11.8282\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 88.2805 - val_loss: 11.9241\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 115.5308 - val_loss: 11.9259\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 81.8614 - val_loss: 11.8641\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 106.4976 - val_loss: 11.9062\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 134.4615 - val_loss: 12.0494\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 66.9756 - val_loss: 12.1042\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 104.1196 - val_loss: 12.0545\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 86.4138 - val_loss: 12.1900\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 71.4633 - val_loss: 12.3567\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 109.6795 - val_loss: 12.6393\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 107.9404 - val_loss: 13.1827\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 141.4524 - val_loss: 13.7129\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 73.8484 - val_loss: 14.5875\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 56.3756 - val_loss: 15.8807\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 114.9922 - val_loss: 15.6222\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 99.2253 - val_loss: 14.8261\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 52.9581 - val_loss: 14.5502\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 84.0379 - val_loss: 14.0227\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 88.7249 - val_loss: 13.4017\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.8753 - val_loss: 13.4533\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 73.6093 - val_loss: 13.5400\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 72.0533 - val_loss: 13.8468\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 79.8588 - val_loss: 14.0064\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 74.3358 - val_loss: 14.2573\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 75.6387 - val_loss: 14.5968\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 64.8447 - val_loss: 15.1312\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 42.3709 - val_loss: 15.4435\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 55.6897 - val_loss: 15.2265\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 78.6973 - val_loss: 14.8776\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 89.4679 - val_loss: 14.6479\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 68.0893 - val_loss: 14.5636\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 38.1985 - val_loss: 14.7230\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 59.5511 - val_loss: 15.0296\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 39.9170 - val_loss: 15.5155\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 59.2622 - val_loss: 15.2630\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 29.6163 - val_loss: 15.0054\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 38.5489 - val_loss: 14.8364\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 84.2888 - val_loss: 14.5468\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 43.0649 - val_loss: 14.4904\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 78.1095 - val_loss: 14.4183\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 32.7741 - val_loss: 14.6034\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 77.7242 - val_loss: 15.1867\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 66.3954 - val_loss: 14.9987\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 67.3170 - val_loss: 14.7900\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 84.6023 - val_loss: 14.7613\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 83.8790 - val_loss: 14.5276\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 58.2121 - val_loss: 14.6587\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 62.1728 - val_loss: 15.1371\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 57.0962 - val_loss: 15.0317\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 114.5851 - val_loss: 14.9605\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 53.2953 - val_loss: 15.5423\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 33.9832 - val_loss: 16.1537\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 56.6768 - val_loss: 16.2847\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 36.0804 - val_loss: 16.3100\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 34.0766 - val_loss: 15.7821\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 46.2535 - val_loss: 14.9131\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 47.0018 - val_loss: 14.4565\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 27.3988 - val_loss: 14.4714\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 47.8434 - val_loss: 14.6609\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 47.0185 - val_loss: 14.7350\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 35.5989 - val_loss: 15.4058\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 60.1697 - val_loss: 15.6052\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 55.5354 - val_loss: 15.0431\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.7862 - val_loss: 14.4661\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.9390 - val_loss: 14.2398\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 37.1950 - val_loss: 14.4554\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 54.3551 - val_loss: 14.7332\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 62.6505 - val_loss: 14.9456\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 39.0791 - val_loss: 14.8282\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 61.8654 - val_loss: 14.4060\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 49.2542 - val_loss: 13.7546\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 43.6615 - val_loss: 13.8557\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 61.2720 - val_loss: 14.5405\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.4496 - val_loss: 15.4203\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 47.5998 - val_loss: 16.1860\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 50.5449 - val_loss: 16.4431\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 54.5903 - val_loss: 15.8325\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 41.0185 - val_loss: 15.3488\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 53.0646 - val_loss: 15.0994\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 40.9089 - val_loss: 14.6553\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 34.7111 - val_loss: 14.5941\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 52.2267 - val_loss: 15.1728\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 42.5076 - val_loss: 16.1482\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 33.9706 - val_loss: 16.7301\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 94.7802 - val_loss: 17.0320\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 50.7201 - val_loss: 17.4694\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.0769 - val_loss: 17.8843\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 44.3095 - val_loss: 17.2740\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 40.7362 - val_loss: 15.9175\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 67.2845 - val_loss: 15.0512\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 42.7615 - val_loss: 14.9373\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 26.9090 - val_loss: 15.0492\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 30.8039 - val_loss: 15.2763\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 21.7953 - val_loss: 15.3456\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 36.5788 - val_loss: 14.7477\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 41.9309 - val_loss: 14.1615\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 35.5465 - val_loss: 13.7339\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 26.3037 - val_loss: 13.5920\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 49.0289 - val_loss: 13.6404\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 43.1344 - val_loss: 13.9282\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 61.3128 - val_loss: 14.5772\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 63.1769 - val_loss: 15.3852\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 28.1770 - val_loss: 16.0971\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 45.2038 - val_loss: 16.2474\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 34.6268 - val_loss: 16.5642\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.1059 - val_loss: 17.0964\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 44.4972 - val_loss: 17.7897\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 31.4265 - val_loss: 17.8548\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 47.0263 - val_loss: 18.0529\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 30.3096 - val_loss: 18.7985\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 36.3239 - val_loss: 18.6094\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 30.9962 - val_loss: 17.9979\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.2287 - val_loss: 18.0197\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 39.3593 - val_loss: 18.0462\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.6124 - val_loss: 18.5248\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 65.7117 - val_loss: 18.3404\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 38.3418 - val_loss: 17.7699\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 38.1323 - val_loss: 17.4410\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 41.1493 - val_loss: 17.5137\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 56.8795 - val_loss: 17.9464\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 57.5413 - val_loss: 19.0090\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 39.3230 - val_loss: 20.6407\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.2992 - val_loss: 20.9146\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 63.2540 - val_loss: 20.5322\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.0392 - val_loss: 20.1041\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 45.1568 - val_loss: 20.2865\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 56.6586 - val_loss: 20.9433\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 20.1426 - val_loss: 21.5080\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 62.2997 - val_loss: 22.0405\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.5126 - val_loss: 22.2163\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 31.5701 - val_loss: 22.0736\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.9600 - val_loss: 22.2466\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 15.4229 - val_loss: 21.9800\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 26.3229 - val_loss: 21.9622\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 43.0684 - val_loss: 21.8712\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 29.4918 - val_loss: 21.4883\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 18.1622 - val_loss: 20.6590\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 32.3924 - val_loss: 19.2648\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 20.8828 - val_loss: 18.8908\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.2857 - val_loss: 18.6546\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 13.8468 - val_loss: 18.3018\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 18.5457 - val_loss: 17.5507\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.3912 - val_loss: 16.2365\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 23.0596 - val_loss: 15.6665\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 51.0056 - val_loss: 15.3724\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 30.5097 - val_loss: 15.0744\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 40.5493 - val_loss: 15.3797\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 33.0619 - val_loss: 15.9594\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 21.0723 - val_loss: 16.5726\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 14.4360 - val_loss: 16.9080\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 35.6886 - val_loss: 16.4045\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.3772 - val_loss: 15.6263\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 45.9907 - val_loss: 15.0453\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 26.9567 - val_loss: 15.0077\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 35.6510 - val_loss: 15.1927\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 26.8861 - val_loss: 15.7490\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 46.0331 - val_loss: 16.6248\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 30.5533 - val_loss: 17.4094\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 58.3055 - val_loss: 18.5645\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 35.3885 - val_loss: 19.1373\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 12.8854 - val_loss: 19.1151\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 32.5171 - val_loss: 18.9790\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 42.9059 - val_loss: 18.8951\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 36.4834 - val_loss: 19.4047\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 37.7638 - val_loss: 20.3997\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 33.5434 - val_loss: 21.2497\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dea6cc0dae0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Évaluer le modèle"
      ],
      "metadata": {
        "id": "vGpvXbCLz7ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss  = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFUHzVdSz9fH",
        "outputId": "f09ace11-1b65-41d6-98c6-443425fa6b5e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "260.5631103515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Regarder les prévisions pour les données de test"
      ],
      "metadata": {
        "id": "T0nQtwR60GDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test, verbose=0)\n",
        "\n",
        "for i in range(len(X_test)):\n",
        " print(X_test[i], y_test[i], y_pred[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY1sPmAn0JGq",
        "outputId": "f2293d58-01d9-4f62-a301-a90af9cdac8b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[21.8   19.5   11.44 ]\n",
            " [20.8    0.    13.41 ]\n",
            " [23.3    0.     9.968]] 5.758 [10.714093]\n",
            "[[20.8    0.    13.41 ]\n",
            " [23.3    0.     9.968]\n",
            " [23.9    0.     5.758]] 3.485 [8.1301565]\n",
            "[[23.3    0.     9.968]\n",
            " [23.9    0.     5.758]\n",
            " [24.8    0.     3.485]] 2.562 [3.2910817]\n",
            "[[23.9    0.     5.758]\n",
            " [24.8    0.     3.485]\n",
            " [22.6   14.5    2.562]] 2.538 [2.9992456]\n",
            "[[24.8    0.     3.485]\n",
            " [22.6   14.5    2.562]\n",
            " [22.6    0.     2.538]] 3.102 [4.5462527]\n",
            "[[22.6   14.5    2.562]\n",
            " [22.6    0.     2.538]\n",
            " [23.4    7.3    3.102]] 18.4 [1.6626779]\n",
            "[[22.6    0.     2.538]\n",
            " [23.4    7.3    3.102]\n",
            " [20.    47.4   18.4  ]] 45.4 [14.025172]\n",
            "[[23.4    7.3    3.102]\n",
            " [20.    47.4   18.4  ]\n",
            " [20.    30.9   45.4  ]] 62.69 [32.37211]\n",
            "[[20.   47.4  18.4 ]\n",
            " [20.   30.9  45.4 ]\n",
            " [20.3   0.   62.69]] 55.89 [57.39104]\n",
            "[[20.   30.9  45.4 ]\n",
            " [20.3   0.   62.69]\n",
            " [18.3  58.1  55.89]] 53.82 [125.25615]\n",
            "[[20.3   0.   62.69]\n",
            " [18.3  58.1  55.89]\n",
            " [21.3   0.   53.82]] 49.78 [72.84215]\n",
            "[[18.3  58.1  55.89]\n",
            " [21.3   0.   53.82]\n",
            " [22.8   0.   49.78]] 39.02 [37.596985]\n",
            "[[21.3   0.   53.82]\n",
            " [22.8   0.   49.78]\n",
            " [20.1  24.4  39.02]] 39.25 [47.050316]\n",
            "[[22.8   0.   49.78]\n",
            " [20.1  24.4  39.02]\n",
            " [22.2   0.1  39.25]] 33.63 [44.46472]\n",
            "[[20.1  24.4  39.02]\n",
            " [22.2   0.1  39.25]\n",
            " [20.3   2.4  33.63]] 21.36 [26.725027]\n",
            "[[22.2   0.1  39.25]\n",
            " [20.3   2.4  33.63]\n",
            " [20.1   1.1  21.36]] 13.94 [18.890833]\n",
            "[[20.3   2.4  33.63]\n",
            " [20.1   1.1  21.36]\n",
            " [20.5   0.   13.94]] 8.779 [13.201237]\n",
            "[[20.1    1.1   21.36 ]\n",
            " [20.5    0.    13.94 ]\n",
            " [18.     6.5    8.779]] 9.718 [10.644049]\n",
            "[[20.5    0.    13.94 ]\n",
            " [18.     6.5    8.779]\n",
            " [19.5    6.6    9.718]] 10.73 [22.25727]\n",
            "[[18.     6.5    8.779]\n",
            " [19.5    6.6    9.718]\n",
            " [20.2    0.    10.73 ]] 9.286 [18.104565]\n",
            "[[19.5    6.6    9.718]\n",
            " [20.2    0.    10.73 ]\n",
            " [20.3    0.     9.286]] 7.464 [10.744745]\n",
            "[[20.2    0.    10.73 ]\n",
            " [20.3    0.     9.286]\n",
            " [20.5    0.     7.464]] 6.291 [6.835345]\n",
            "[[20.3    0.     9.286]\n",
            " [20.5    0.     7.464]\n",
            " [19.85   0.     6.291]] 5.438 [4.7069135]\n",
            "[[20.5    0.     7.464]\n",
            " [19.85   0.     6.291]\n",
            " [22.6    1.     5.438]] 5.994 [4.211385]\n",
            "[[19.85   0.     6.291]\n",
            " [22.6    1.     5.438]\n",
            " [21.8   35.7    5.994]] 6.074 [8.97291]\n",
            "[[22.6    1.     5.438]\n",
            " [21.8   35.7    5.994]\n",
            " [16.     0.     6.074]] 5.497 [5.03264]\n",
            "[[21.8   35.7    5.994]\n",
            " [16.     0.     6.074]\n",
            " [16.     0.     5.497]] 5.776 [3.2199647]\n",
            "[[16.     0.     6.074]\n",
            " [16.     0.     5.497]\n",
            " [16.1    8.8    5.776]] 5.11 [5.1354594]\n",
            "[[16.     0.     5.497]\n",
            " [16.1    8.8    5.776]\n",
            " [15.2    0.     5.11 ]] 3.651 [4.9054275]\n",
            "[[16.1    8.8    5.776]\n",
            " [15.2    0.     5.11 ]\n",
            " [14.7    0.     3.651]] 3.636 [2.9074898]\n",
            "[[15.2    0.     5.11 ]\n",
            " [14.7    0.     3.651]\n",
            " [15.     9.2    3.636]] 12.38 [2.054181]\n",
            "[[14.7    0.     3.651]\n",
            " [15.     9.2    3.636]\n",
            " [17.6   10.6   12.38 ]] 15.68 [15.360994]\n",
            "[[15.     9.2    3.636]\n",
            " [17.6   10.6   12.38 ]\n",
            " [17.5    0.    15.68 ]] 10.12 [18.34109]\n",
            "[[17.6  10.6  12.38]\n",
            " [17.5   0.   15.68]\n",
            " [16.7   0.   10.12]] 7.821 [10.679349]\n",
            "[[17.5    0.    15.68 ]\n",
            " [16.7    0.    10.12 ]\n",
            " [16.5    3.9    7.821]] 31.46 [10.13442]\n",
            "[[16.7    0.    10.12 ]\n",
            " [16.5    3.9    7.821]\n",
            " [16.5   77.9   31.46 ]] 39.06 [58.46267]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Résumé du modèle"
      ],
      "metadata": {
        "id": "06dGPI5O0YVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLd6Wccs0aEL",
        "outputId": "b868e432-b177-4026-81b0-aab7e8e25032"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 150)               92400     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 50)                7550      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 102551 (400.59 KB)\n",
            "Trainable params: 102551 (400.59 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 3: Série multidimensionnelles: nous allons maintenant prédire simultanément les température, précipitations et débits simultanément en fonction des valeurs passés"
      ],
      "metadata": {
        "id": "4AeF2mRY1ISd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Fonction servant à formatter les données d'une séquence multiple"
      ],
      "metadata": {
        "id": "pR-WgGq41jG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sequence3(sequence, n_steps):\n",
        " X, y = list(), list()\n",
        " for i in range(len(sequence)):\n",
        " # find the end of this pattern\n",
        "  end_ix = i + n_steps\n",
        " # check if we are beyond the sequence\n",
        "  if end_ix > len(sequence)-1:\n",
        "    break\n",
        " # gather input and output parts of the pattern\n",
        "  seq_x, seq_y = sequence[i:end_ix,:], sequence[end_ix,:]\n",
        "  X.append(seq_x)\n",
        "  y.append(seq_y)\n",
        " return array(X), array(y)"
      ],
      "metadata": {
        "id": "mNpibqOV1nFw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Formatter les données à l'aide de la fonction précédente"
      ],
      "metadata": {
        "id": "6-CBSkvQ14bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 3\n",
        "\n",
        "X, y = split_sequence3(raw_seq2, n_steps)\n",
        "\n",
        "n_features = 3\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "for i in range(len(X)):\n",
        " print(X[i], y[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zeZqneG170d",
        "outputId": "f80c4c03-1759-4db3-a012-b43460ecb1d5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(116, 3, 3) (116, 3)\n",
            "[[ 7.2   0.   17.41]\n",
            " [ 6.4   0.   25.37]\n",
            " [ 5.7   0.   30.38]] [ 7.6   0.   36.74]\n",
            "[[ 6.4   0.   25.37]\n",
            " [ 5.7   0.   30.38]\n",
            " [ 7.6   0.   36.74]] [ 5.9 11.8 51.5]\n",
            "[[ 5.7   0.   30.38]\n",
            " [ 7.6   0.   36.74]\n",
            " [ 5.9  11.8  51.5 ]] [ 5.2   0.7  69.64]\n",
            "[[ 7.6   0.   36.74]\n",
            " [ 5.9  11.8  51.5 ]\n",
            " [ 5.2   0.7  69.64]] [ 4.9   1.7  72.08]\n",
            "[[ 5.9  11.8  51.5 ]\n",
            " [ 5.2   0.7  69.64]\n",
            " [ 4.9   1.7  72.08]] [ 2.6   0.   57.16]\n",
            "[[ 5.2   0.7  69.64]\n",
            " [ 4.9   1.7  72.08]\n",
            " [ 2.6   0.   57.16]] [ 3.5   0.   42.24]\n",
            "[[ 4.9   1.7  72.08]\n",
            " [ 2.6   0.   57.16]\n",
            " [ 3.5   0.   42.24]] [10.2   0.   34.46]\n",
            "[[ 2.6   0.   57.16]\n",
            " [ 3.5   0.   42.24]\n",
            " [10.2   0.   34.46]] [ 8.9   9.3  37.96]\n",
            "[[ 3.5   0.   42.24]\n",
            " [10.2   0.   34.46]\n",
            " [ 8.9   9.3  37.96]] [ 5.1   8.9  48.31]\n",
            "[[10.2   0.   34.46]\n",
            " [ 8.9   9.3  37.96]\n",
            " [ 5.1   8.9  48.31]] [ 6.2   3.1  58.88]\n",
            "[[ 8.9   9.3  37.96]\n",
            " [ 5.1   8.9  48.31]\n",
            " [ 6.2   3.1  58.88]] [ 8.7   3.1  53.85]\n",
            "[[ 5.1   8.9  48.31]\n",
            " [ 6.2   3.1  58.88]\n",
            " [ 8.7   3.1  53.85]] [ 6.4   0.   45.15]\n",
            "[[ 6.2   3.1  58.88]\n",
            " [ 8.7   3.1  53.85]\n",
            " [ 6.4   0.   45.15]] [ 8.5   0.   37.22]\n",
            "[[ 8.7   3.1  53.85]\n",
            " [ 6.4   0.   45.15]\n",
            " [ 8.5   0.   37.22]] [ 9.2   0.2  31.66]\n",
            "[[ 6.4   0.   45.15]\n",
            " [ 8.5   0.   37.22]\n",
            " [ 9.2   0.2  31.66]] [ 9.   16.7  32.68]\n",
            "[[ 8.5   0.   37.22]\n",
            " [ 9.2   0.2  31.66]\n",
            " [ 9.   16.7  32.68]] [ 7.2 24.6 42.2]\n",
            "[[ 9.2   0.2  31.66]\n",
            " [ 9.   16.7  32.68]\n",
            " [ 7.2  24.6  42.2 ]] [ 9.2   0.   63.83]\n",
            "[[ 9.   16.7  32.68]\n",
            " [ 7.2  24.6  42.2 ]\n",
            " [ 9.2   0.   63.83]] [ 6.5   0.2  55.17]\n",
            "[[ 7.2  24.6  42.2 ]\n",
            " [ 9.2   0.   63.83]\n",
            " [ 6.5   0.2  55.17]] [ 8.4   1.4  38.87]\n",
            "[[ 9.2   0.   63.83]\n",
            " [ 6.5   0.2  55.17]\n",
            " [ 8.4   1.4  38.87]] [12.4   0.   24.79]\n",
            "[[ 6.5   0.2  55.17]\n",
            " [ 8.4   1.4  38.87]\n",
            " [12.4   0.   24.79]] [15.1   0.   17.85]\n",
            "[[ 8.4   1.4  38.87]\n",
            " [12.4   0.   24.79]\n",
            " [15.1   0.   17.85]] [13.    0.   15.75]\n",
            "[[12.4   0.   24.79]\n",
            " [15.1   0.   17.85]\n",
            " [13.    0.   15.75]] [ 7.4   0.   13.19]\n",
            "[[15.1   0.   17.85]\n",
            " [13.    0.   15.75]\n",
            " [ 7.4   0.   13.19]] [7.    0.    9.438]\n",
            "[[13.     0.    15.75 ]\n",
            " [ 7.4    0.    13.19 ]\n",
            " [ 7.     0.     9.438]] [12.3   0.    8.42]\n",
            "[[ 7.4    0.    13.19 ]\n",
            " [ 7.     0.     9.438]\n",
            " [12.3    0.     8.42 ]] [13.2    0.     6.156]\n",
            "[[ 7.     0.     9.438]\n",
            " [12.3    0.     8.42 ]\n",
            " [13.2    0.     6.156]] [16.2    0.     5.106]\n",
            "[[12.3    0.     8.42 ]\n",
            " [13.2    0.     6.156]\n",
            " [16.2    0.     5.106]] [9.1   0.    6.626]\n",
            "[[13.2    0.     6.156]\n",
            " [16.2    0.     5.106]\n",
            " [ 9.1    0.     6.626]] [11.4   0.    5.63]\n",
            "[[16.2    0.     5.106]\n",
            " [ 9.1    0.     6.626]\n",
            " [11.4    0.     5.63 ]] [12.9    0.     4.519]\n",
            "[[ 9.1    0.     6.626]\n",
            " [11.4    0.     5.63 ]\n",
            " [12.9    0.     4.519]] [5.3   3.1   4.226]\n",
            "[[11.4    0.     5.63 ]\n",
            " [12.9    0.     4.519]\n",
            " [ 5.3    3.1    4.226]] [3.2   0.    4.337]\n",
            "[[12.9    0.     4.519]\n",
            " [ 5.3    3.1    4.226]\n",
            " [ 3.2    0.     4.337]] [5.6  0.   3.58]\n",
            "[[5.3   3.1   4.226]\n",
            " [3.2   0.    4.337]\n",
            " [5.6   0.    3.58 ]] [11.     0.     3.411]\n",
            "[[ 3.2    0.     4.337]\n",
            " [ 5.6    0.     3.58 ]\n",
            " [11.     0.     3.411]] [17.1    4.8    3.469]\n",
            "[[ 5.6    0.     3.58 ]\n",
            " [11.     0.     3.411]\n",
            " [17.1    4.8    3.469]] [12.    18.3    6.423]\n",
            "[[11.     0.     3.411]\n",
            " [17.1    4.8    3.469]\n",
            " [12.    18.3    6.423]] [6.7  0.   7.25]\n",
            "[[17.1    4.8    3.469]\n",
            " [12.    18.3    6.423]\n",
            " [ 6.7    0.     7.25 ]] [11.7    0.     5.621]\n",
            "[[12.    18.3    6.423]\n",
            " [ 6.7    0.     7.25 ]\n",
            " [11.7    0.     5.621]] [10.2    0.7    4.085]\n",
            "[[ 6.7    0.     7.25 ]\n",
            " [11.7    0.     5.621]\n",
            " [10.2    0.7    4.085]] [10.     0.     3.882]\n",
            "[[11.7    0.     5.621]\n",
            " [10.2    0.7    4.085]\n",
            " [10.     0.     3.882]] [11.6    0.     3.391]\n",
            "[[10.2    0.7    4.085]\n",
            " [10.     0.     3.882]\n",
            " [11.6    0.     3.391]] [15.8    0.     3.082]\n",
            "[[10.     0.     3.882]\n",
            " [11.6    0.     3.391]\n",
            " [15.8    0.     3.082]] [20.6    0.     2.755]\n",
            "[[11.6    0.     3.391]\n",
            " [15.8    0.     3.082]\n",
            " [20.6    0.     2.755]] [12.3   0.    2.36]\n",
            "[[15.8    0.     3.082]\n",
            " [20.6    0.     2.755]\n",
            " [12.3    0.     2.36 ]] [15.3    0.     2.055]\n",
            "[[20.6    0.     2.755]\n",
            " [12.3    0.     2.36 ]\n",
            " [15.3    0.     2.055]] [18.6    0.     1.831]\n",
            "[[12.3    0.     2.36 ]\n",
            " [15.3    0.     2.055]\n",
            " [18.6    0.     1.831]] [23.1   11.3    1.961]\n",
            "[[15.3    0.     2.055]\n",
            " [18.6    0.     1.831]\n",
            " [23.1   11.3    1.961]] [14.1    0.     2.449]\n",
            "[[18.6    0.     1.831]\n",
            " [23.1   11.3    1.961]\n",
            " [14.1    0.     2.449]] [10.1    0.     2.189]\n",
            "[[23.1   11.3    1.961]\n",
            " [14.1    0.     2.449]\n",
            " [10.1    0.     2.189]] [10.3    0.     1.857]\n",
            "[[14.1    0.     2.449]\n",
            " [10.1    0.     2.189]\n",
            " [10.3    0.     1.857]] [11.8   0.    1.59]\n",
            "[[10.1    0.     2.189]\n",
            " [10.3    0.     1.857]\n",
            " [11.8    0.     1.59 ]] [12.7    3.     1.617]\n",
            "[[10.3    0.     1.857]\n",
            " [11.8    0.     1.59 ]\n",
            " [12.7    3.     1.617]] [11.7    4.9    1.908]\n",
            "[[11.8    0.     1.59 ]\n",
            " [12.7    3.     1.617]\n",
            " [11.7    4.9    1.908]] [12.4   0.    1.93]\n",
            "[[12.7    3.     1.617]\n",
            " [11.7    4.9    1.908]\n",
            " [12.4    0.     1.93 ]] [13.2    0.5    1.987]\n",
            "[[11.7    4.9    1.908]\n",
            " [12.4    0.     1.93 ]\n",
            " [13.2    0.5    1.987]] [14.6    0.7    1.525]\n",
            "[[12.4    0.     1.93 ]\n",
            " [13.2    0.5    1.987]\n",
            " [14.6    0.7    1.525]] [17.7    0.     1.478]\n",
            "[[13.2    0.5    1.987]\n",
            " [14.6    0.7    1.525]\n",
            " [17.7    0.     1.478]] [19.3    0.     1.261]\n",
            "[[14.6    0.7    1.525]\n",
            " [17.7    0.     1.478]\n",
            " [19.3    0.     1.261]] [16.2   14.5    1.832]\n",
            "[[17.7    0.     1.478]\n",
            " [19.3    0.     1.261]\n",
            " [16.2   14.5    1.832]] [17.4    4.4    2.562]\n",
            "[[19.3    0.     1.261]\n",
            " [16.2   14.5    1.832]\n",
            " [17.4    4.4    2.562]] [14.1    8.1    3.053]\n",
            "[[16.2   14.5    1.832]\n",
            " [17.4    4.4    2.562]\n",
            " [14.1    8.1    3.053]] [14.5    0.     3.262]\n",
            "[[17.4    4.4    2.562]\n",
            " [14.1    8.1    3.053]\n",
            " [14.5    0.     3.262]] [13.1   14.1    3.716]\n",
            "[[14.1    8.1    3.053]\n",
            " [14.5    0.     3.262]\n",
            " [13.1   14.1    3.716]] [13.4    1.7    3.651]\n",
            "[[14.5    0.     3.262]\n",
            " [13.1   14.1    3.716]\n",
            " [13.4    1.7    3.651]] [14.5    2.7    3.067]\n",
            "[[13.1   14.1    3.716]\n",
            " [13.4    1.7    3.651]\n",
            " [14.5    2.7    3.067]] [15.9    0.2    2.844]\n",
            "[[13.4    1.7    3.651]\n",
            " [14.5    2.7    3.067]\n",
            " [15.9    0.2    2.844]] [17.9    0.     2.336]\n",
            "[[14.5    2.7    3.067]\n",
            " [15.9    0.2    2.844]\n",
            " [17.9    0.     2.336]] [18.3   0.    1.84]\n",
            "[[15.9    0.2    2.844]\n",
            " [17.9    0.     2.336]\n",
            " [18.3    0.     1.84 ]] [19.5    0.     1.528]\n",
            "[[17.9    0.     2.336]\n",
            " [18.3    0.     1.84 ]\n",
            " [19.5    0.     1.528]] [20.6    0.     1.514]\n",
            "[[18.3    0.     1.84 ]\n",
            " [19.5    0.     1.528]\n",
            " [20.6    0.     1.514]] [20.5    0.     1.554]\n",
            "[[19.5    0.     1.528]\n",
            " [20.6    0.     1.514]\n",
            " [20.5    0.     1.554]] [17.1    2.1    1.146]\n",
            "[[20.6    0.     1.514]\n",
            " [20.5    0.     1.554]\n",
            " [17.1    2.1    1.146]] [19.6    3.7    1.404]\n",
            "[[20.5    0.     1.554]\n",
            " [17.1    2.1    1.146]\n",
            " [19.6    3.7    1.404]] [22.2   10.8    2.057]\n",
            "[[17.1    2.1    1.146]\n",
            " [19.6    3.7    1.404]\n",
            " [22.2   10.8    2.057]] [17.5   3.    6.54]\n",
            "[[19.6    3.7    1.404]\n",
            " [22.2   10.8    2.057]\n",
            " [17.5    3.     6.54 ]] [20.3   0.   10.23]\n",
            "[[22.2   10.8    2.057]\n",
            " [17.5    3.     6.54 ]\n",
            " [20.3    0.    10.23 ]] [21.5    0.4    7.732]\n",
            "[[17.5    3.     6.54 ]\n",
            " [20.3    0.    10.23 ]\n",
            " [21.5    0.4    7.732]] [21.8  19.5  11.44]\n",
            "[[20.3    0.    10.23 ]\n",
            " [21.5    0.4    7.732]\n",
            " [21.8   19.5   11.44 ]] [20.8   0.   13.41]\n",
            "[[21.5    0.4    7.732]\n",
            " [21.8   19.5   11.44 ]\n",
            " [20.8    0.    13.41 ]] [23.3    0.     9.968]\n",
            "[[21.8   19.5   11.44 ]\n",
            " [20.8    0.    13.41 ]\n",
            " [23.3    0.     9.968]] [23.9    0.     5.758]\n",
            "[[20.8    0.    13.41 ]\n",
            " [23.3    0.     9.968]\n",
            " [23.9    0.     5.758]] [24.8    0.     3.485]\n",
            "[[23.3    0.     9.968]\n",
            " [23.9    0.     5.758]\n",
            " [24.8    0.     3.485]] [22.6   14.5    2.562]\n",
            "[[23.9    0.     5.758]\n",
            " [24.8    0.     3.485]\n",
            " [22.6   14.5    2.562]] [22.6    0.     2.538]\n",
            "[[24.8    0.     3.485]\n",
            " [22.6   14.5    2.562]\n",
            " [22.6    0.     2.538]] [23.4    7.3    3.102]\n",
            "[[22.6   14.5    2.562]\n",
            " [22.6    0.     2.538]\n",
            " [23.4    7.3    3.102]] [20.  47.4 18.4]\n",
            "[[22.6    0.     2.538]\n",
            " [23.4    7.3    3.102]\n",
            " [20.    47.4   18.4  ]] [20.  30.9 45.4]\n",
            "[[23.4    7.3    3.102]\n",
            " [20.    47.4   18.4  ]\n",
            " [20.    30.9   45.4  ]] [20.3   0.   62.69]\n",
            "[[20.   47.4  18.4 ]\n",
            " [20.   30.9  45.4 ]\n",
            " [20.3   0.   62.69]] [18.3  58.1  55.89]\n",
            "[[20.   30.9  45.4 ]\n",
            " [20.3   0.   62.69]\n",
            " [18.3  58.1  55.89]] [21.3   0.   53.82]\n",
            "[[20.3   0.   62.69]\n",
            " [18.3  58.1  55.89]\n",
            " [21.3   0.   53.82]] [22.8   0.   49.78]\n",
            "[[18.3  58.1  55.89]\n",
            " [21.3   0.   53.82]\n",
            " [22.8   0.   49.78]] [20.1  24.4  39.02]\n",
            "[[21.3   0.   53.82]\n",
            " [22.8   0.   49.78]\n",
            " [20.1  24.4  39.02]] [22.2   0.1  39.25]\n",
            "[[22.8   0.   49.78]\n",
            " [20.1  24.4  39.02]\n",
            " [22.2   0.1  39.25]] [20.3   2.4  33.63]\n",
            "[[20.1  24.4  39.02]\n",
            " [22.2   0.1  39.25]\n",
            " [20.3   2.4  33.63]] [20.1   1.1  21.36]\n",
            "[[22.2   0.1  39.25]\n",
            " [20.3   2.4  33.63]\n",
            " [20.1   1.1  21.36]] [20.5   0.   13.94]\n",
            "[[20.3   2.4  33.63]\n",
            " [20.1   1.1  21.36]\n",
            " [20.5   0.   13.94]] [18.     6.5    8.779]\n",
            "[[20.1    1.1   21.36 ]\n",
            " [20.5    0.    13.94 ]\n",
            " [18.     6.5    8.779]] [19.5    6.6    9.718]\n",
            "[[20.5    0.    13.94 ]\n",
            " [18.     6.5    8.779]\n",
            " [19.5    6.6    9.718]] [20.2   0.   10.73]\n",
            "[[18.     6.5    8.779]\n",
            " [19.5    6.6    9.718]\n",
            " [20.2    0.    10.73 ]] [20.3    0.     9.286]\n",
            "[[19.5    6.6    9.718]\n",
            " [20.2    0.    10.73 ]\n",
            " [20.3    0.     9.286]] [20.5    0.     7.464]\n",
            "[[20.2    0.    10.73 ]\n",
            " [20.3    0.     9.286]\n",
            " [20.5    0.     7.464]] [19.85   0.     6.291]\n",
            "[[20.3    0.     9.286]\n",
            " [20.5    0.     7.464]\n",
            " [19.85   0.     6.291]] [22.6    1.     5.438]\n",
            "[[20.5    0.     7.464]\n",
            " [19.85   0.     6.291]\n",
            " [22.6    1.     5.438]] [21.8   35.7    5.994]\n",
            "[[19.85   0.     6.291]\n",
            " [22.6    1.     5.438]\n",
            " [21.8   35.7    5.994]] [16.     0.     6.074]\n",
            "[[22.6    1.     5.438]\n",
            " [21.8   35.7    5.994]\n",
            " [16.     0.     6.074]] [16.     0.     5.497]\n",
            "[[21.8   35.7    5.994]\n",
            " [16.     0.     6.074]\n",
            " [16.     0.     5.497]] [16.1    8.8    5.776]\n",
            "[[16.     0.     6.074]\n",
            " [16.     0.     5.497]\n",
            " [16.1    8.8    5.776]] [15.2   0.    5.11]\n",
            "[[16.     0.     5.497]\n",
            " [16.1    8.8    5.776]\n",
            " [15.2    0.     5.11 ]] [14.7    0.     3.651]\n",
            "[[16.1    8.8    5.776]\n",
            " [15.2    0.     5.11 ]\n",
            " [14.7    0.     3.651]] [15.     9.2    3.636]\n",
            "[[15.2    0.     5.11 ]\n",
            " [14.7    0.     3.651]\n",
            " [15.     9.2    3.636]] [17.6  10.6  12.38]\n",
            "[[14.7    0.     3.651]\n",
            " [15.     9.2    3.636]\n",
            " [17.6   10.6   12.38 ]] [17.5   0.   15.68]\n",
            "[[15.     9.2    3.636]\n",
            " [17.6   10.6   12.38 ]\n",
            " [17.5    0.    15.68 ]] [16.7   0.   10.12]\n",
            "[[17.6  10.6  12.38]\n",
            " [17.5   0.   15.68]\n",
            " [16.7   0.   10.12]] [16.5    3.9    7.821]\n",
            "[[17.5    0.    15.68 ]\n",
            " [16.7    0.    10.12 ]\n",
            " [16.5    3.9    7.821]] [16.5  77.9  31.46]\n",
            "[[16.7    0.    10.12 ]\n",
            " [16.5    3.9    7.821]\n",
            " [16.5   77.9   31.46 ]] [17.5   0.   39.06]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Diviser les données en train, valid et test data"
      ],
      "metadata": {
        "id": "bC72sasx29Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = X[:70], y[:70]\n",
        "X_valid, y_valid = X[70:80], y[70:80]\n",
        "X_test, y_test = X[80:], y[80:]\n",
        "\n",
        "print(X_train.shape, X_valid.shape, X_test.shape)\n",
        "\n",
        "for i in range(len(X_train)):\n",
        " print(X_train[i], y_train[i])\n",
        "\n",
        "#for i in range(len(X_test)):\n",
        "# print(X_test[i], y_test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hrFghi33ANQ",
        "outputId": "a2a2407d-6912-4a73-9007-b49d081cb4c7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 3, 3) (10, 3, 3) (36, 3, 3)\n",
            "[[ 7.2   0.   17.41]\n",
            " [ 6.4   0.   25.37]\n",
            " [ 5.7   0.   30.38]] [ 7.6   0.   36.74]\n",
            "[[ 6.4   0.   25.37]\n",
            " [ 5.7   0.   30.38]\n",
            " [ 7.6   0.   36.74]] [ 5.9 11.8 51.5]\n",
            "[[ 5.7   0.   30.38]\n",
            " [ 7.6   0.   36.74]\n",
            " [ 5.9  11.8  51.5 ]] [ 5.2   0.7  69.64]\n",
            "[[ 7.6   0.   36.74]\n",
            " [ 5.9  11.8  51.5 ]\n",
            " [ 5.2   0.7  69.64]] [ 4.9   1.7  72.08]\n",
            "[[ 5.9  11.8  51.5 ]\n",
            " [ 5.2   0.7  69.64]\n",
            " [ 4.9   1.7  72.08]] [ 2.6   0.   57.16]\n",
            "[[ 5.2   0.7  69.64]\n",
            " [ 4.9   1.7  72.08]\n",
            " [ 2.6   0.   57.16]] [ 3.5   0.   42.24]\n",
            "[[ 4.9   1.7  72.08]\n",
            " [ 2.6   0.   57.16]\n",
            " [ 3.5   0.   42.24]] [10.2   0.   34.46]\n",
            "[[ 2.6   0.   57.16]\n",
            " [ 3.5   0.   42.24]\n",
            " [10.2   0.   34.46]] [ 8.9   9.3  37.96]\n",
            "[[ 3.5   0.   42.24]\n",
            " [10.2   0.   34.46]\n",
            " [ 8.9   9.3  37.96]] [ 5.1   8.9  48.31]\n",
            "[[10.2   0.   34.46]\n",
            " [ 8.9   9.3  37.96]\n",
            " [ 5.1   8.9  48.31]] [ 6.2   3.1  58.88]\n",
            "[[ 8.9   9.3  37.96]\n",
            " [ 5.1   8.9  48.31]\n",
            " [ 6.2   3.1  58.88]] [ 8.7   3.1  53.85]\n",
            "[[ 5.1   8.9  48.31]\n",
            " [ 6.2   3.1  58.88]\n",
            " [ 8.7   3.1  53.85]] [ 6.4   0.   45.15]\n",
            "[[ 6.2   3.1  58.88]\n",
            " [ 8.7   3.1  53.85]\n",
            " [ 6.4   0.   45.15]] [ 8.5   0.   37.22]\n",
            "[[ 8.7   3.1  53.85]\n",
            " [ 6.4   0.   45.15]\n",
            " [ 8.5   0.   37.22]] [ 9.2   0.2  31.66]\n",
            "[[ 6.4   0.   45.15]\n",
            " [ 8.5   0.   37.22]\n",
            " [ 9.2   0.2  31.66]] [ 9.   16.7  32.68]\n",
            "[[ 8.5   0.   37.22]\n",
            " [ 9.2   0.2  31.66]\n",
            " [ 9.   16.7  32.68]] [ 7.2 24.6 42.2]\n",
            "[[ 9.2   0.2  31.66]\n",
            " [ 9.   16.7  32.68]\n",
            " [ 7.2  24.6  42.2 ]] [ 9.2   0.   63.83]\n",
            "[[ 9.   16.7  32.68]\n",
            " [ 7.2  24.6  42.2 ]\n",
            " [ 9.2   0.   63.83]] [ 6.5   0.2  55.17]\n",
            "[[ 7.2  24.6  42.2 ]\n",
            " [ 9.2   0.   63.83]\n",
            " [ 6.5   0.2  55.17]] [ 8.4   1.4  38.87]\n",
            "[[ 9.2   0.   63.83]\n",
            " [ 6.5   0.2  55.17]\n",
            " [ 8.4   1.4  38.87]] [12.4   0.   24.79]\n",
            "[[ 6.5   0.2  55.17]\n",
            " [ 8.4   1.4  38.87]\n",
            " [12.4   0.   24.79]] [15.1   0.   17.85]\n",
            "[[ 8.4   1.4  38.87]\n",
            " [12.4   0.   24.79]\n",
            " [15.1   0.   17.85]] [13.    0.   15.75]\n",
            "[[12.4   0.   24.79]\n",
            " [15.1   0.   17.85]\n",
            " [13.    0.   15.75]] [ 7.4   0.   13.19]\n",
            "[[15.1   0.   17.85]\n",
            " [13.    0.   15.75]\n",
            " [ 7.4   0.   13.19]] [7.    0.    9.438]\n",
            "[[13.     0.    15.75 ]\n",
            " [ 7.4    0.    13.19 ]\n",
            " [ 7.     0.     9.438]] [12.3   0.    8.42]\n",
            "[[ 7.4    0.    13.19 ]\n",
            " [ 7.     0.     9.438]\n",
            " [12.3    0.     8.42 ]] [13.2    0.     6.156]\n",
            "[[ 7.     0.     9.438]\n",
            " [12.3    0.     8.42 ]\n",
            " [13.2    0.     6.156]] [16.2    0.     5.106]\n",
            "[[12.3    0.     8.42 ]\n",
            " [13.2    0.     6.156]\n",
            " [16.2    0.     5.106]] [9.1   0.    6.626]\n",
            "[[13.2    0.     6.156]\n",
            " [16.2    0.     5.106]\n",
            " [ 9.1    0.     6.626]] [11.4   0.    5.63]\n",
            "[[16.2    0.     5.106]\n",
            " [ 9.1    0.     6.626]\n",
            " [11.4    0.     5.63 ]] [12.9    0.     4.519]\n",
            "[[ 9.1    0.     6.626]\n",
            " [11.4    0.     5.63 ]\n",
            " [12.9    0.     4.519]] [5.3   3.1   4.226]\n",
            "[[11.4    0.     5.63 ]\n",
            " [12.9    0.     4.519]\n",
            " [ 5.3    3.1    4.226]] [3.2   0.    4.337]\n",
            "[[12.9    0.     4.519]\n",
            " [ 5.3    3.1    4.226]\n",
            " [ 3.2    0.     4.337]] [5.6  0.   3.58]\n",
            "[[5.3   3.1   4.226]\n",
            " [3.2   0.    4.337]\n",
            " [5.6   0.    3.58 ]] [11.     0.     3.411]\n",
            "[[ 3.2    0.     4.337]\n",
            " [ 5.6    0.     3.58 ]\n",
            " [11.     0.     3.411]] [17.1    4.8    3.469]\n",
            "[[ 5.6    0.     3.58 ]\n",
            " [11.     0.     3.411]\n",
            " [17.1    4.8    3.469]] [12.    18.3    6.423]\n",
            "[[11.     0.     3.411]\n",
            " [17.1    4.8    3.469]\n",
            " [12.    18.3    6.423]] [6.7  0.   7.25]\n",
            "[[17.1    4.8    3.469]\n",
            " [12.    18.3    6.423]\n",
            " [ 6.7    0.     7.25 ]] [11.7    0.     5.621]\n",
            "[[12.    18.3    6.423]\n",
            " [ 6.7    0.     7.25 ]\n",
            " [11.7    0.     5.621]] [10.2    0.7    4.085]\n",
            "[[ 6.7    0.     7.25 ]\n",
            " [11.7    0.     5.621]\n",
            " [10.2    0.7    4.085]] [10.     0.     3.882]\n",
            "[[11.7    0.     5.621]\n",
            " [10.2    0.7    4.085]\n",
            " [10.     0.     3.882]] [11.6    0.     3.391]\n",
            "[[10.2    0.7    4.085]\n",
            " [10.     0.     3.882]\n",
            " [11.6    0.     3.391]] [15.8    0.     3.082]\n",
            "[[10.     0.     3.882]\n",
            " [11.6    0.     3.391]\n",
            " [15.8    0.     3.082]] [20.6    0.     2.755]\n",
            "[[11.6    0.     3.391]\n",
            " [15.8    0.     3.082]\n",
            " [20.6    0.     2.755]] [12.3   0.    2.36]\n",
            "[[15.8    0.     3.082]\n",
            " [20.6    0.     2.755]\n",
            " [12.3    0.     2.36 ]] [15.3    0.     2.055]\n",
            "[[20.6    0.     2.755]\n",
            " [12.3    0.     2.36 ]\n",
            " [15.3    0.     2.055]] [18.6    0.     1.831]\n",
            "[[12.3    0.     2.36 ]\n",
            " [15.3    0.     2.055]\n",
            " [18.6    0.     1.831]] [23.1   11.3    1.961]\n",
            "[[15.3    0.     2.055]\n",
            " [18.6    0.     1.831]\n",
            " [23.1   11.3    1.961]] [14.1    0.     2.449]\n",
            "[[18.6    0.     1.831]\n",
            " [23.1   11.3    1.961]\n",
            " [14.1    0.     2.449]] [10.1    0.     2.189]\n",
            "[[23.1   11.3    1.961]\n",
            " [14.1    0.     2.449]\n",
            " [10.1    0.     2.189]] [10.3    0.     1.857]\n",
            "[[14.1    0.     2.449]\n",
            " [10.1    0.     2.189]\n",
            " [10.3    0.     1.857]] [11.8   0.    1.59]\n",
            "[[10.1    0.     2.189]\n",
            " [10.3    0.     1.857]\n",
            " [11.8    0.     1.59 ]] [12.7    3.     1.617]\n",
            "[[10.3    0.     1.857]\n",
            " [11.8    0.     1.59 ]\n",
            " [12.7    3.     1.617]] [11.7    4.9    1.908]\n",
            "[[11.8    0.     1.59 ]\n",
            " [12.7    3.     1.617]\n",
            " [11.7    4.9    1.908]] [12.4   0.    1.93]\n",
            "[[12.7    3.     1.617]\n",
            " [11.7    4.9    1.908]\n",
            " [12.4    0.     1.93 ]] [13.2    0.5    1.987]\n",
            "[[11.7    4.9    1.908]\n",
            " [12.4    0.     1.93 ]\n",
            " [13.2    0.5    1.987]] [14.6    0.7    1.525]\n",
            "[[12.4    0.     1.93 ]\n",
            " [13.2    0.5    1.987]\n",
            " [14.6    0.7    1.525]] [17.7    0.     1.478]\n",
            "[[13.2    0.5    1.987]\n",
            " [14.6    0.7    1.525]\n",
            " [17.7    0.     1.478]] [19.3    0.     1.261]\n",
            "[[14.6    0.7    1.525]\n",
            " [17.7    0.     1.478]\n",
            " [19.3    0.     1.261]] [16.2   14.5    1.832]\n",
            "[[17.7    0.     1.478]\n",
            " [19.3    0.     1.261]\n",
            " [16.2   14.5    1.832]] [17.4    4.4    2.562]\n",
            "[[19.3    0.     1.261]\n",
            " [16.2   14.5    1.832]\n",
            " [17.4    4.4    2.562]] [14.1    8.1    3.053]\n",
            "[[16.2   14.5    1.832]\n",
            " [17.4    4.4    2.562]\n",
            " [14.1    8.1    3.053]] [14.5    0.     3.262]\n",
            "[[17.4    4.4    2.562]\n",
            " [14.1    8.1    3.053]\n",
            " [14.5    0.     3.262]] [13.1   14.1    3.716]\n",
            "[[14.1    8.1    3.053]\n",
            " [14.5    0.     3.262]\n",
            " [13.1   14.1    3.716]] [13.4    1.7    3.651]\n",
            "[[14.5    0.     3.262]\n",
            " [13.1   14.1    3.716]\n",
            " [13.4    1.7    3.651]] [14.5    2.7    3.067]\n",
            "[[13.1   14.1    3.716]\n",
            " [13.4    1.7    3.651]\n",
            " [14.5    2.7    3.067]] [15.9    0.2    2.844]\n",
            "[[13.4    1.7    3.651]\n",
            " [14.5    2.7    3.067]\n",
            " [15.9    0.2    2.844]] [17.9    0.     2.336]\n",
            "[[14.5    2.7    3.067]\n",
            " [15.9    0.2    2.844]\n",
            " [17.9    0.     2.336]] [18.3   0.    1.84]\n",
            "[[15.9    0.2    2.844]\n",
            " [17.9    0.     2.336]\n",
            " [18.3    0.     1.84 ]] [19.5    0.     1.528]\n",
            "[[17.9    0.     2.336]\n",
            " [18.3    0.     1.84 ]\n",
            " [19.5    0.     1.528]] [20.6    0.     1.514]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Définir le modèle LSTM multidimendionnel"
      ],
      "metadata": {
        "id": "sb3aWV9C3Lzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(150, activation='relu', kernel_initializer=\"he_normal\",input_shape=(n_steps, n_features)))\n",
        "model.add(Dense(50,activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50,activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(n_features)) #ATTENTION: changement ici pour prédire 3 valeurs\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "6EMEQUEH3QmT"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Entrainer le modèle"
      ],
      "metadata": {
        "id": "3e0mdv4r3aSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=1, validation_data=[X_valid,y_valid])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjbVpbWG3b6K",
        "outputId": "dda92032-efe3-4cd8-a37c-e57a5e78404b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3/3 [==============================] - 2s 164ms/step - loss: 1921.1163 - val_loss: 490.1615\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1305.4266 - val_loss: 232.8055\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 640.8924 - val_loss: 127.0215\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 476.5834 - val_loss: 83.4631\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 330.0291 - val_loss: 62.6808\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 307.3355 - val_loss: 52.3038\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 294.0230 - val_loss: 46.5662\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 257.2898 - val_loss: 42.9985\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 264.4865 - val_loss: 40.8326\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 201.7749 - val_loss: 39.5403\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 186.8880 - val_loss: 38.6116\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 150.5486 - val_loss: 38.8837\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 142.1132 - val_loss: 38.9736\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 102.9896 - val_loss: 37.3397\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 126.9586 - val_loss: 36.3503\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 173.8912 - val_loss: 36.2239\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 103.3726 - val_loss: 35.6157\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 142.7077 - val_loss: 35.8955\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 115.1282 - val_loss: 36.0766\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 89.4568 - val_loss: 35.1885\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 87.5399 - val_loss: 34.0271\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 87.0544 - val_loss: 32.7744\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 108.7781 - val_loss: 31.2053\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 110.0315 - val_loss: 30.3121\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 101.3672 - val_loss: 29.7781\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 84.2909 - val_loss: 29.4826\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 70.6936 - val_loss: 29.4189\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 91.1115 - val_loss: 29.3510\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 80.1295 - val_loss: 29.3676\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 76.7778 - val_loss: 29.0550\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 97.7257 - val_loss: 29.2030\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 94.9625 - val_loss: 29.4758\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 66.7174 - val_loss: 29.5313\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 75.3480 - val_loss: 29.1889\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 68.6935 - val_loss: 28.6728\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 75.1112 - val_loss: 28.0073\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 71.1154 - val_loss: 27.5181\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 53.8722 - val_loss: 27.7447\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 53.6975 - val_loss: 27.7852\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 72.3364 - val_loss: 27.7542\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 61.7433 - val_loss: 28.1415\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 66.6687 - val_loss: 27.8082\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 48.6789 - val_loss: 27.5831\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 58.3121 - val_loss: 27.1185\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 48.2840 - val_loss: 26.4309\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 60.0905 - val_loss: 25.7678\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 55.4369 - val_loss: 25.9227\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 41.4147 - val_loss: 26.1247\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.9349 - val_loss: 26.2117\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 48.9701 - val_loss: 26.4083\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 46.6938 - val_loss: 26.2249\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 51.8345 - val_loss: 25.8443\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 47.7949 - val_loss: 25.8701\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 52.0642 - val_loss: 27.0634\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 57.4545 - val_loss: 28.3632\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 66.9004 - val_loss: 29.1016\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 43.9946 - val_loss: 29.7391\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 47.2184 - val_loss: 29.9106\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 54.8081 - val_loss: 29.5198\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 47.0413 - val_loss: 29.1043\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 45.5094 - val_loss: 29.1174\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 40.2481 - val_loss: 28.9199\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 45.6932 - val_loss: 28.9976\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 44.8042 - val_loss: 29.9535\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 63.5885 - val_loss: 30.1078\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 55.1959 - val_loss: 30.1812\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 48.8921 - val_loss: 30.0612\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 37.0980 - val_loss: 30.4419\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 38.1592 - val_loss: 30.2862\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 44.8062 - val_loss: 29.2939\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 44.2571 - val_loss: 29.1994\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 45.1456 - val_loss: 30.5157\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 45.6581 - val_loss: 31.3117\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 39.3606 - val_loss: 31.0790\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 44.2627 - val_loss: 30.5323\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 33.0595 - val_loss: 30.2244\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.1287 - val_loss: 29.6768\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.9255 - val_loss: 28.9092\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 56.3149 - val_loss: 28.2339\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 50.2854 - val_loss: 28.4491\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 38.5868 - val_loss: 29.4020\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 42.4246 - val_loss: 30.7297\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 34.5686 - val_loss: 32.2546\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 37.2643 - val_loss: 33.4852\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 31.1041 - val_loss: 33.6512\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 41.2877 - val_loss: 33.7967\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 43.1519 - val_loss: 33.2601\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 42.0294 - val_loss: 32.9581\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 34.1944 - val_loss: 33.1400\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 43.2764 - val_loss: 32.7173\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 45.8526 - val_loss: 31.7158\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 35.6542 - val_loss: 30.8174\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 34.9507 - val_loss: 29.7671\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 39.2253 - val_loss: 29.3940\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 41.1519 - val_loss: 29.6005\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 40.9316 - val_loss: 30.7142\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 49.5780 - val_loss: 31.5319\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 32.3767 - val_loss: 31.6630\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 39.1966 - val_loss: 32.1260\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 25.3834 - val_loss: 32.3040\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 40.0219 - val_loss: 32.3141\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 28.9586 - val_loss: 32.6881\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 32.3964 - val_loss: 32.6266\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 34.1092 - val_loss: 32.1763\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 30.9077 - val_loss: 31.0693\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 34.6327 - val_loss: 30.3450\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 37.2788 - val_loss: 30.3839\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 28.9501 - val_loss: 31.9046\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 27.7467 - val_loss: 32.4445\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 32.5572 - val_loss: 33.4757\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 37.9647 - val_loss: 35.1109\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 34.7667 - val_loss: 35.0668\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 27.4879 - val_loss: 34.2331\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 33.8542 - val_loss: 32.2683\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 44.4774 - val_loss: 30.1027\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 27.6773 - val_loss: 28.7018\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 33.8185 - val_loss: 28.4783\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 24.7839 - val_loss: 28.7179\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 37.4400 - val_loss: 29.2175\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 30.4693 - val_loss: 30.2563\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 37.0342 - val_loss: 32.0995\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 24.7307 - val_loss: 34.1782\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 38.0418 - val_loss: 36.4610\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 31.0340 - val_loss: 38.1572\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 31.5889 - val_loss: 39.0809\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.3489 - val_loss: 39.3073\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 28.4477 - val_loss: 38.8434\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 41.0274 - val_loss: 38.5767\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 33.4736 - val_loss: 38.1115\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 44.9857 - val_loss: 37.2444\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.1119 - val_loss: 37.5912\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 32.7694 - val_loss: 38.1857\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 39.6682 - val_loss: 37.5252\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.1266 - val_loss: 35.8791\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.4187 - val_loss: 34.1383\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 34.2053 - val_loss: 33.7015\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 29.9055 - val_loss: 34.5623\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.0936 - val_loss: 34.4055\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 22.0959 - val_loss: 34.8273\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 26.7440 - val_loss: 35.3591\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 41.3583 - val_loss: 34.9259\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 26.2909 - val_loss: 34.7688\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 27.7010 - val_loss: 35.0410\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 25.6316 - val_loss: 35.8985\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.4486 - val_loss: 37.4390\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 30.7767 - val_loss: 38.6410\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 28.4851 - val_loss: 39.9868\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.8806 - val_loss: 41.1637\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 24.3309 - val_loss: 42.2750\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 24.0727 - val_loss: 42.9325\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 23.6988 - val_loss: 43.0670\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 34.5798 - val_loss: 42.7403\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.3199 - val_loss: 42.7832\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 35.6776 - val_loss: 43.0405\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.1060 - val_loss: 43.3427\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 36.4220 - val_loss: 43.1382\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.5364 - val_loss: 41.8229\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 23.2038 - val_loss: 40.5124\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 27.2528 - val_loss: 40.6087\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 23.3648 - val_loss: 41.1804\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 27.6824 - val_loss: 41.2582\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 24.5710 - val_loss: 40.9778\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 34.5240 - val_loss: 41.0457\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 27.1730 - val_loss: 41.4783\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 24.4509 - val_loss: 42.2395\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 23.4735 - val_loss: 43.4997\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 27.6872 - val_loss: 44.6611\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.9442 - val_loss: 45.4259\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 28.1477 - val_loss: 45.6501\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 21.8905 - val_loss: 44.8615\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 21.1824 - val_loss: 43.5674\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 18.9061 - val_loss: 42.6751\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 22.2624 - val_loss: 42.2128\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 36.7779 - val_loss: 41.6429\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.9814 - val_loss: 40.6860\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 27.3729 - val_loss: 40.3452\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 36.2511 - val_loss: 40.8987\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 18.1317 - val_loss: 41.5427\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 20.3482 - val_loss: 41.8982\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.7436 - val_loss: 41.8021\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 22.5200 - val_loss: 41.4916\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 42.9524 - val_loss: 41.7394\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 34.1242 - val_loss: 41.9054\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.9562 - val_loss: 41.9133\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.1544 - val_loss: 41.9998\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 31.7453 - val_loss: 41.7716\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 27.6314 - val_loss: 41.7017\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 25.5726 - val_loss: 42.2486\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 26.3649 - val_loss: 42.9794\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 25.4114 - val_loss: 43.0307\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3182 - val_loss: 43.4570\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 28.8831 - val_loss: 44.4723\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 26.2845 - val_loss: 45.1524\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 16.9406 - val_loss: 45.0656\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 33.5616 - val_loss: 44.4710\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.0292 - val_loss: 43.9050\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 27.4024 - val_loss: 44.0041\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 25.1834 - val_loss: 44.5810\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 22.0022 - val_loss: 45.6428\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 20.7395 - val_loss: 45.6627\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dea6ea456f0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Évaluer le modèle sur les données de test"
      ],
      "metadata": {
        "id": "3Kbwt9Hz3obq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss  = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FM7MgNd3qPo",
        "outputId": "aad1c53a-b7d8-459d-cae6-c700b522b793"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196.72198486328125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Regarder les prévisions sur les données de test"
      ],
      "metadata": {
        "id": "Nzs3mNXI3zQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test, verbose=0)\n",
        "\n",
        "for i in range(len(X_test)):\n",
        " print(X_test[i], y_test[i], y_pred[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9oPUss832e1",
        "outputId": "5fcb7c52-6855-45a3-ffe9-42adb01b3183"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[21.8   19.5   11.44 ]\n",
            " [20.8    0.    13.41 ]\n",
            " [23.3    0.     9.968]] [23.9    0.     5.758] [11.085297   -0.24132307  5.764153  ]\n",
            "[[20.8    0.    13.41 ]\n",
            " [23.3    0.     9.968]\n",
            " [23.9    0.     5.758]] [24.8    0.     3.485] [14.289177   -0.15459812  3.7340105 ]\n",
            "[[23.3    0.     9.968]\n",
            " [23.9    0.     5.758]\n",
            " [24.8    0.     3.485]] [22.6   14.5    2.562] [23.330206   -0.21951516  5.0423822 ]\n",
            "[[23.9    0.     5.758]\n",
            " [24.8    0.     3.485]\n",
            " [22.6   14.5    2.562]] [22.6    0.     2.538] [15.139746   2.0697207  2.5483274]\n",
            "[[24.8    0.     3.485]\n",
            " [22.6   14.5    2.562]\n",
            " [22.6    0.     2.538]] [23.4    7.3    3.102] [17.459303  13.93686    1.2016383]\n",
            "[[22.6   14.5    2.562]\n",
            " [22.6    0.     2.538]\n",
            " [23.4    7.3    3.102]] [20.  47.4 18.4] [22.18488    3.6268375  2.5329344]\n",
            "[[22.6    0.     2.538]\n",
            " [23.4    7.3    3.102]\n",
            " [20.    47.4   18.4  ]] [20.  30.9 45.4] [12.543207 10.439823  9.130147]\n",
            "[[23.4    7.3    3.102]\n",
            " [20.    47.4   18.4  ]\n",
            " [20.    30.9   45.4  ]] [20.3   0.   62.69] [ 6.5812054  -0.33132136 27.212833  ]\n",
            "[[20.   47.4  18.4 ]\n",
            " [20.   30.9  45.4 ]\n",
            " [20.3   0.   62.69]] [18.3  58.1  55.89] [ 9.166136  3.51296  54.186375]\n",
            "[[20.   30.9  45.4 ]\n",
            " [20.3   0.   62.69]\n",
            " [18.3  58.1  55.89]] [21.3   0.   53.82] [ 3.6332123  1.1305702 73.17221  ]\n",
            "[[20.3   0.   62.69]\n",
            " [18.3  58.1  55.89]\n",
            " [21.3   0.   53.82]] [22.8   0.   49.78] [22.6267   -2.033093 57.012356]\n",
            "[[18.3  58.1  55.89]\n",
            " [21.3   0.   53.82]\n",
            " [22.8   0.   49.78]] [20.1  24.4  39.02] [22.00569  -2.691853 30.326572]\n",
            "[[21.3   0.   53.82]\n",
            " [22.8   0.   49.78]\n",
            " [20.1  24.4  39.02]] [22.2   0.1  39.25] [ 4.23515  12.069567 55.74577 ]\n",
            "[[22.8   0.   49.78]\n",
            " [20.1  24.4  39.02]\n",
            " [22.2   0.1  39.25]] [20.3   2.4  33.63] [ 9.76422   1.142644 31.268082]\n",
            "[[20.1  24.4  39.02]\n",
            " [22.2   0.1  39.25]\n",
            " [20.3   2.4  33.63]] [20.1   1.1  21.36] [14.563572   1.1243254 22.402277 ]\n",
            "[[22.2   0.1  39.25]\n",
            " [20.3   2.4  33.63]\n",
            " [20.1   1.1  21.36]] [20.5   0.   13.94] [23.453335   1.5758249 14.910974 ]\n",
            "[[20.3   2.4  33.63]\n",
            " [20.1   1.1  21.36]\n",
            " [20.5   0.   13.94]] [18.     6.5    8.779] [28.915281  -0.8396641  7.4909716]\n",
            "[[20.1    1.1   21.36 ]\n",
            " [20.5    0.    13.94 ]\n",
            " [18.     6.5    8.779]] [19.5    6.6    9.718] [9.773595 4.706759 9.549093]\n",
            "[[20.5    0.    13.94 ]\n",
            " [18.     6.5    8.779]\n",
            " [19.5    6.6    9.718]] [20.2   0.   10.73] [10.651381   6.2027683 16.750917 ]\n",
            "[[18.     6.5    8.779]\n",
            " [19.5    6.6    9.718]\n",
            " [20.2    0.    10.73 ]] [20.3    0.     9.286] [8.4564705  0.41617614 7.631219  ]\n",
            "[[19.5    6.6    9.718]\n",
            " [20.2    0.    10.73 ]\n",
            " [20.3    0.     9.286]] [20.5    0.     7.464] [4.923886  0.2919904 8.9261675]\n",
            "[[20.2    0.    10.73 ]\n",
            " [20.3    0.     9.286]\n",
            " [20.5    0.     7.464]] [19.85   0.     6.291] [ 7.945833   -0.80711794  6.43773   ]\n",
            "[[20.3    0.     9.286]\n",
            " [20.5    0.     7.464]\n",
            " [19.85   0.     6.291]] [22.6    1.     5.438] [15.385663  -0.5583117  5.1293354]\n",
            "[[20.5    0.     7.464]\n",
            " [19.85   0.     6.291]\n",
            " [22.6    1.     5.438]] [21.8   35.7    5.994] [16.784775    0.03034295  4.6014    ]\n",
            "[[19.85   0.     6.291]\n",
            " [22.6    1.     5.438]\n",
            " [21.8   35.7    5.994]] [16.     0.     6.074] [8.7764435 1.8590586 6.2017097]\n",
            "[[22.6    1.     5.438]\n",
            " [21.8   35.7    5.994]\n",
            " [16.     0.     6.074]] [16.     0.     5.497] [17.145933   0.9996103  4.769903 ]\n",
            "[[21.8   35.7    5.994]\n",
            " [16.     0.     6.074]\n",
            " [16.     0.     5.497]] [16.1    8.8    5.776] [25.960218 -4.219331  7.594469]\n",
            "[[16.     0.     6.074]\n",
            " [16.     0.     5.497]\n",
            " [16.1    8.8    5.776]] [15.2   0.    5.11] [8.957274  7.642405  7.0578575]\n",
            "[[16.     0.     5.497]\n",
            " [16.1    8.8    5.776]\n",
            " [15.2    0.     5.11 ]] [14.7    0.     3.651] [14.932772   2.5741673  5.117577 ]\n",
            "[[16.1    8.8    5.776]\n",
            " [15.2    0.     5.11 ]\n",
            " [14.7    0.     3.651]] [15.     9.2    3.636] [13.568753   -0.26966715  4.092385  ]\n",
            "[[15.2    0.     5.11 ]\n",
            " [14.7    0.     3.651]\n",
            " [15.     9.2    3.636]] [17.6  10.6  12.38] [7.550493  2.574884  3.3003345]\n",
            "[[14.7    0.     3.651]\n",
            " [15.     9.2    3.636]\n",
            " [17.6   10.6   12.38 ]] [17.5   0.   15.68] [ 7.240886   4.0636463 13.896438 ]\n",
            "[[15.     9.2    3.636]\n",
            " [17.6   10.6   12.38 ]\n",
            " [17.5    0.    15.68 ]] [16.7   0.   10.12] [4.135052  1.5625868 9.122828 ]\n",
            "[[17.6  10.6  12.38]\n",
            " [17.5   0.   15.68]\n",
            " [16.7   0.   10.12]] [16.5    3.9    7.821] [11.557573   1.1073895  6.471034 ]\n",
            "[[17.5    0.    15.68 ]\n",
            " [16.7    0.    10.12 ]\n",
            " [16.5    3.9    7.821]] [16.5  77.9  31.46] [10.071713  4.756541 11.218021]\n",
            "[[16.7    0.    10.12 ]\n",
            " [16.5    3.9    7.821]\n",
            " [16.5   77.9   31.46 ]] [17.5   0.   39.06] [ 2.004044   -0.54636383 24.617813  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Résumé du modèle"
      ],
      "metadata": {
        "id": "_62rAodQ4bem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8HDTTLH4deR",
        "outputId": "c8e9448f-83ff-48bc-8bd0-a3f019bfae52"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 150)               92400     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 50)                7550      \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 3)                 153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 102653 (400.99 KB)\n",
            "Trainable params: 102653 (400.99 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}